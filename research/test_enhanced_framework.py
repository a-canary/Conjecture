#!/usr/bin/env python3
"""
Test script for enhanced research framework
Validates baseline comparison and statistical analysis functionality
"""

import asyncio
import json
import sys
import os
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from experiments.baseline_comparison import BaselineEngine, BaselineType, BaselineComparisonSuite
from analysis.statistical_analyzer import StatisticalAnalyzer
from experiments.llm_judge import LLMJudge, EvaluationCriterion


async def test_baseline_engine():
    """Test baseline engine functionality"""
    print("üß™ Testing Baseline Engine...")
    
    # Mock LLM manager for testing
    class MockLLMManager:
        async def generate_response(self, prompt, model, max_tokens=2000, temperature=0.7):
            # Simple mock response based on prompt length and type
            if "step by step" in prompt.lower():
                return "Let me think through this step by step: First, I need to understand the question. Then I'll analyze the components. Finally, I'll provide a comprehensive answer."
            elif "examples:" in prompt.lower():
                return "Based on the examples provided, I can see the pattern. Here's my answer to the current question."
            else:
                return f"This is a mock response to the question about using {model}. The response would normally be generated by the actual model."
    
    mock_manager = MockLLMManager()
    engine = BaselineEngine(mock_manager)
    
    # Test each baseline type
    question = "What are the benefits of renewable energy?"
    
    for baseline_type in BaselineType:
        config = engine.baseline_configs[baseline_type]
        result = await engine.execute_baseline(config, question, "test-model")
        
        print(f"  ‚úÖ {baseline_type.value}: {len(result.response)} chars response Generated")
        print(f"      Execution time: {result.execution_time_seconds:.3f}s")
        
        if result.error:
            print(f"      ‚ö†Ô∏è  Error: {result.error}")
    
    print("‚úÖ Baseline engine tests completed\n")
    return True


async def test_statistical_analyzer():
    """Test statistical analyzer functionality"""
    print("üß™ Testing Statistical Analyzer...")
    
    analyzer = StatisticalAnalyzer()
    
    # Test with sample data
    group_a = [0.8, 0.7, 0.9, 0.75, 0.85, 0.72, 0.88, 0.79]  # Conjecture scores
    group_b = [0.6, 0.65, 0.7, 0.58, 0.68, 0.62, 0.71, 0.64]  # Baseline scores
    
    # Test A/B analysis
    result = analyzer.analyze_ab_test(
        group_a_scores=group_a,
        group_b_scores=group_b,
        comparison_name="test_comparison",
        is_paired=True
    )
    
    print(f"  ‚úÖ A/B Test Analysis:")
    print(f"      Mean A (Conjecture): {result.mean_a:.3f}")
    print(f"      Mean B (Baseline): {result.mean_b:.3f}")
    print(f"      Mean Difference: {result.mean_difference:+.3f}")
    print(f"      Recommendation: {result.recommendation}")
    
    # Test individual statistical tests
    pa_t_test = analyzer.paired_t_test(group_a, group_b)
    print(f"  ‚úÖ Paired t-test: p = {pa_t_test.p_value:.4f}, significant = {pa_t_test.is_significant}")
    
    wilcoxon_test = analyzer.wilcoxon_signed_rank_test(group_a, group_b)
    print(f"  ‚úÖ Wilcoxon test: p = {wilcoxon_test.p_value:.4f}, significant = {wilcoxon_test.is_significant}")
    
    # Test effect size calculations
    cohens_d = analyzer.calculate_cohens_d(group_a, group_b)
    hedges_g = analyzer.calculate_hedges_g(group_a, group_b)
    print(f"  ‚úÖ Effect sizes: Cohen's d = {cohens_d:.3f}, Hedges' g = {hedges_g:.3f}")
    
    print("‚úÖ Statistical analyzer tests completed\n")
    return True


async def test_baseline_comparison_structure():
    """Test baseline comparison suite structure without full execution"""
    print("üß™ Testing Baseline Comparison Suite Structure...")
    
    # Mock framework
    class MockFramework:
        class MockLLMManager:
            async def generate_response(self, prompt, model, max_tokens=2000, temperature=0.7):
                return "Mock response for testing"
        
        def __init__(self):
            self.llm_manager = self.MockLLMManager()
    
    mock_framework = MockFramework()
    
    # Initialize baseline comparison suite
    try:
        suite = BaselineComparisonSuite(mock_framework)
        
        # Test initialization
        print(f"  ‚úÖ BaselineComparisonSuite initialized")
        print(f"      Judge model: {suite.judge.judge_model}")
        print(f"      Baseline types available: {list(suite.baseline_engine.baseline_configs.keys())}")
        print(f"      Statistical analyzer initialized: {suite.statistical_analyzer is not None}")
        
        # Test baseline configurations
        for baseline_type in BaselineType:
            config = suite.baseline_engine.baseline_configs[baseline_type]
            print(f"      {baseline_type.value}: {config.name}")
        
        print("‚úÖ Baseline comparison suite structure tests completed\n")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Error initializing baseline comparison suite: {e}")
        return False


async def test_configuration():
    """Test configuration loading"""
    print("üß™ Testing Configuration...")
    
    # Test research config
    config_path = Path(__file__).parent / "config.json"
    
    if config_path.exists():
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            print(f"  ‚úÖ Research config loaded successfully")
            print(f"      Judge model: {config.get('judge_model', 'not set')}")
            print(f"      Provider count: {len(config.get('providers', []))}")
            print(f"      Baseline comparison enabled: {config.get('experiments', {}).get('baseline_comparison', 'not set')}")
            
        except Exception as e:
            print(f"  ‚ùå Error loading research config: {e}")
            return False
    else:
        print(f"  ‚ö†Ô∏è  Research config not found at {config_path}")
    
    # Test .env example
    env_example_path = Path(__file__).parent / ".env.example"
    
    if env_example_path.exists():
        with open(env_example_path, 'r') as f:
            env_content = f.read()
        
        has_judge_model = "JUDGE_MODEL=" in env_content
        has_baseline_comparison = "BASELINE_COMPARISON=" in env_content
        
        print(f"  ‚úÖ .env.example contains JUDGE_MODEL: {has_judge_model}")
        print(f"  ‚úÖ .env.example contains BASELINE_COMPARISON: {has_baseline_comparison}")
    
    print("‚úÖ Configuration tests completed\n")
    return True


async def test_integration():
    """Test integration of all components"""
    print("üß™ Testing Component Integration...")
    
    try:
        # Test import chain
        from experiments.baseline_comparison import BaselineEngine, BaselineType, BaselineComparisonSuite
        from analysis.statistical_analyzer import StatisticalAnalyzer
        from experiments.llm_judge import LLMJudge, EvaluationCriterion
        
        print("  ‚úÖ All imports successful")
        
        # Test enum values
        baseline_types = list(BaselineType)
        criteria = list(EvaluationCriterion)
        
        print(f"  ‚úÖ Baseline types: {len(baseline_types)} configured")
        print(f"  ‚úÖ Evaluation criteria: {len(criteria)} configured")
        
        # Test analyzer instantiation
        analyzer = StatisticalAnalyzer(alpha=0.05, effect_size_threshold=0.2)
        print(f"  ‚úÖ Statistical analyzer: Œ±={analyzer.alpha}, threshold={analyzer.effect_size_threshold}")
        
        print("‚úÖ Component integration tests completed\n")
        return True
        
    except Exception as e:
        print(f"  ‚ùå Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all validation tests"""
    print("üöÄ Starting Enhanced Research Framework Validation\n")
    print("=" * 60)
    
    tests = [
        ("Configuration", test_configuration),
        ("Statistical Analyzer", test_statistical_analyzer),
        ("Baseline Engine", test_baseline_engine),
        ("Component Integration", test_integration),
        ("Baseline Comparison Suite", test_baseline_comparison_structure),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"Running {test_name} tests...")
        try:
            results[test_name] = await test_func()
        except Exception as e:
            print(f"‚ùå {test_name} tests failed with exception: {e}")
            import traceback
            traceback.print_exc()
            results[test_name] = False
    
    # Summary
    print("=" * 60)
    print("üìä VALIDATION SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:.<40} {status}")
    
    print(f"\nOverall Result: {passed}/{total} test suites passed")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED - Enhanced research framework is ready!")
        print("\nNext steps:")
        print("1. Configure your API keys in .env file")
        print("2. Run: python research/run_research.py --baseline")
        print("3. Review results in research/analysis/")
    else:
        print("‚ö†Ô∏è  Some tests failed - please review the issues above")
        print("Check that all dependencies are installed:")
        print("pip install -r requirements.txt")
    
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())