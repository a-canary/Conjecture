Starting pytest run on tests/ directory...
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:149: UserWarning: Field "model_settings" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:184: UserWarning: Field name "name" shadows an attribute in parent "Operation"; 
  warnings.warn(
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:184: UserWarning: Field name "metadata" shadows an attribute in parent "Operation"; 
  warnings.warn(
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:184: UserWarning: Field name "done" shadows an attribute in parent "Operation"; 
  warnings.warn(
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:184: UserWarning: Field name "error" shadows an attribute in parent "Operation"; 
  warnings.warn(
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\_internal\_fields.py:149: UserWarning: Field "model_settings" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
===================== test session starts =====================
platform win32 -- Python 3.11.9, pytest-8.4.2, pluggy-1.6.0 -- C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\python.exe
cachedir: .pytest_cache
benchmark: 5.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Optimized Test Suite - Parallel Execution Enabled
Static Analysis Integration: Configured
rootdir: D:\projects\Conjecture\tests
configfile: pytest.ini
plugins: anyio-4.11.0, deepeval-3.7.4, Faker-38.2.0, asyncio-1.2.0, benchmark-5.2.3, cov-7.0.0, mock-3.15.1, repeat-0.9.4, rerunfailures-16.1, timeout-2.4.0, xdist-3.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 235 items

tests\test_claim_models.py::TestClaimModel::test_minimal_claim_creation PASSED [  0%]
tests\test_claim_models.py::TestClaimModel::test_full_claim_creation PASSED [  0%]
tests\test_claim_models.py::TestClaimModel::test_claim_content_validation PASSED [  1%]
tests\test_claim_models.py::TestClaimModel::test_confidence_validation PASSED [  1%]
tests\test_claim_models.py::TestClaimModel::test_tags_validation_and_deduplication PASSED [  2%]
tests\test_claim_models.py::TestClaimModel::test_timestamp_validation PASSED [  2%]
tests\test_claim_models.py::TestClaimModel::test_claim_format_methods PASSED [  2%]
tests\test_claim_models.py::TestClaimModel::test_claim_hash_and_equality PASSED [  3%]
tests\test_claim_processing.py::TestClaimProcessing::test_update_confidence_valid PASSED [  3%]
tests\test_claim_processing.py::TestClaimProcessing::test_update_confidence_invalid PASSED [  4%]
tests\test_claim_processing.py::TestClaimProcessing::test_add_support_new PASSED [  4%]
tests\test_claim_processing.py::TestClaimProcessing::test_add_support_duplicate PASSED [  5%]
tests\test_claim_processing.py::TestClaimProcessing::test_add_supports_new PASSED [  5%]
tests\test_claim_processing.py::TestClaimProcessing::test_mark_dirty_with_reason PASSED [  5%]
tests\test_claim_processing.py::TestClaimProcessing::test_mark_clean PASSED [  6%]
tests\test_claim_processing.py::TestClaimProcessing::test_set_dirty_priority PASSED [  6%]
tests\test_claim_processing.py::TestClaimProcessing::test_processing_chain PASSED [  7%]
tests\test_claim_processing.py::TestClaimProcessing::test_immutability_of_original PASSED [  7%]
tests\test_claim_processing.py::TestClaimProcessing::test_confidence_boundary_values PASSED [  8%]
tests\test_claim_processing.py::TestClaimProcessing::test_dirty_reason_types PASSED [  8%]
tests\test_claim_relationships.py::TestClaimRelationships::test_simple_relationship_creation PASSED [  8%]
tests\test_claim_relationships.py::TestClaimRelationships::test_bidirectional_relationships PASSED [  9%]
tests\test_claim_relationships.py::TestClaimRelationships::test_multiple_supporters PASSED [  9%]
tests\test_claim_relationships.py::TestClaimRelationships::test_multiple_supported_claims PASSED [ 10%]
tests\test_claim_relationships.py::TestClaimRelationships::test_relationship_validation_valid PASSED [ 10%]
tests\test_claim_relationships.py::TestClaimRelationships::test_relationship_validation_missing_supporter PASSED [ 11%]
tests\test_claim_relationships.py::TestClaimRelationships::test_relationship_validation_missing_supported PASSED [ 11%]
tests\test_claim_relationships.py::TestClaimRelationships::test_support_strength_calculation PASSED [ 11%]
tests\test_claim_relationships.py::TestClaimRelationships::test_support_strength_no_supporters PASSED [ 12%]
tests\test_claim_relationships.py::TestClaimRelationships::test_circular_relationships PASSED [ 12%]
tests\test_claim_relationships.py::TestClaimRelationships::test_self_relationship_prevention FAILED [ 13%]
tests\test_claim_relationships.py::TestClaimRelationships::test_complex_relationship_network PASSED [ 13%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_initial_dirty_state PASSED [ 14%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_dirty_reason_assignment PASSED [ 14%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_clean_claim_properties PASSED [ 14%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_state_transitions PASSED [ 15%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_confidence_based_state_logic PASSED [ 15%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_dirty_timestamp_behavior PASSED [ 16%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_dirty_priority_levels PASSED [ 16%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_state_and_dirty_combinations PASSED [ 17%]
tests\test_claim_state_transitions.py::TestClaimStateTransitions::test_updated_timestamp_on_changes PASSED [ 17%]
tests\test_e2e_claim_lifecycle_fixed.py::TestClaimLifecycleE2EFixed::test_complete_claim_lifecycle PASSED [ 17%]
tests\test_e2e_claim_lifecycle_fixed.py::TestClaimLifecycleE2EFixed::test_dirty_flag_propagation_cascade PASSED [ 18%]
tests\test_e2e_claim_lifecycle_fixed.py::TestClaimLifecycleE2EFixed::test_batch_processing_workflow FAILED [ 18%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_minimal_configuration_processing FAILED [ 19%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_performance_configuration_processing FAILED [ 19%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_local_providers_configuration FAILED [ 20%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_error_handling PASSED [ 20%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_fallback_mechanisms PASSED [ 20%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_provider_fallback_and_error_handling FAILED [ 21%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_driven_batch_sizes FAILED [ 21%]
tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_memory_and_disk_usage FAILED [ 22%]
tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_claim_network_reasoning FAILED [ 22%]
tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_confidence_propagation FAILED [ 22%]
tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_batch_reasoning_workflow FAILED [ 23%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestJudgeInitialization::test_successful_initialization PASSED [ 23%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestJudgeInitialization::test_incomplete_config_raises_error PASSED [ 24%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestJudgeInitialization::test_empty_config_raises_error PASSED [ 24%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEnhancedEvaluationPrompts::test_mathematical_evaluation_prompt PASSED [ 25%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEnhancedEvaluationPrompts::test_logical_evaluation_prompt PASSED [ 25%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEnhancedEvaluationPrompts::test_difficulty_specific_guidance PASSED [ 25%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEvaluationParsing::test_json_parsing_success PASSED [ 26%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEvaluationParsing::test_text_parsing_fallback PASSED [ 26%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEvaluationParsing::test_malformed_json_handling PASSED [ 27%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestFallbackEvaluation::test_fallback_evaluation_correct FAILED [ 27%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestFallbackEvaluation::test_fallback_evaluation_incorrect PASSED [ 28%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestCachingFunctionality::test_caching_mechanism PASSED [ 28%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEvaluationSummary::test_empty_evaluations_summary PASSED [ 28%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestEvaluationSummary::test_evaluation_summary_statistics PASSED [ 29%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestBenchmarkComparison::test_enhanced_vs_standard_benchmark PASSED [ 29%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestErrorHandling::test_api_error_handling PASSED [ 30%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestErrorHandling::test_timeout_handling FAILED [ 30%]
tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestErrorHandling::test_malformed_response_handling PASSED [ 31%]
tests\test_enhanced_glm46_judge.py::TestJudgePerformance::test_evaluation_performance ERROR [ 31%]
tests\test_enhanced_glm46_judge.py::TestJudgePerformance::test_batch_evaluation_performance ERROR [ 31%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_mathematical_detection PASSED [ 32%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_logical_detection FAILED [ 32%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_scientific_detection FAILED [ 33%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_sequential_detection PASSED [ 33%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_general_detection PASSED [ 34%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDifficultyEstimation::test_easy_detection FAILED [ 34%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDifficultyEstimation::test_hard_detection PASSED [ 34%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDifficultyEstimation::test_medium_detection PASSED [ 35%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDomainAdaptivePrompts::test_mathematical_prompt PASSED [ 35%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDomainAdaptivePrompts::test_logical_prompt PASSED [ 36%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDomainAdaptivePrompts::test_difficulty_guidance PASSED [ 36%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestEnhancementFunctionality::test_mathematical_reasoning_enhancement PASSED [ 37%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestEnhancementFunctionality::test_multistep_reasoning_enhancement PASSED [ 37%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestEnhancementFunctionality::test_problem_decomposition_enhancement PASSED [ 37%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestIntegrationFunctionality::test_process_with_context_basic PASSED [ 38%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestIntegrationFunctionality::test_enhancement_application PASSED [ 38%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestIntegrationFunctionality::test_caching_functionality PASSED [ 39%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestEnhancementControls::test_enhancement_disable_enable PASSED [ 39%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestResponseParsing::test_mathematical_parsing FAILED [ 40%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestResponseParsing::test_logical_parsing PASSED [ 40%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestResponseParsing::test_general_parsing PASSED [ 40%]
tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestLegacyCompatibility::test_prompt_builder_compatibility PASSED [ 41%]
tests\test_enhanced_prompt_system.py::TestPromptSystemPerformance::test_response_time_performance ERROR [ 41%]
tests\test_enhanced_prompt_system.py::TestPromptSystemPerformance::test_memory_efficiency ERROR [ 42%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestHellaSwagSamples::test_hellaswag_sample_structure PASSED [ 42%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestHellaSwagSamples::test_hellaswag_sample_content PASSED [ 42%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestHellaSwagSamples::test_hellaswag_sample_limit PASSED [ 43%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestMMLUSamples::test_mmlu_sample_structure PASSED [ 43%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestMMLUSamples::test_mmlu_domain_coverage PASSED [ 44%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestMMLUSamples::test_mmlu_metadata_structure PASSED [ 44%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestGSM8KSamples::test_gsm8k_sample_structure PASSED [ 45%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestGSM8KSamples::test_gsm8k_math_problems FAILED [ 45%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestGSM8KSamples::test_gsm8k_metadata_grade_level FAILED [ 45%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestARCSamples::test_arc_sample_structure PASSED [ 46%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestARCSamples::test_arc_science_content PASSED [ 46%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestARCSamples::test_arc_metadata_subject PASSED [ 47%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBigBenchHardSamples::test_bbh_sample_structure PASSED [ 47%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBigBenchHardSamples::test_bbh_logical_reasoning PASSED [ 48%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBigBenchHardSamples::test_bbh_task_type_metadata PASSED [ 48%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_structure FAILED [ 48%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_answer_extraction FAILED [ 49%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_confidence_calculation FAILED [ 49%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_correctness_evaluation FAILED [ 50%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_error_handling FAILED [ 50%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_run_all_benchmarks_structure FAILED [ 51%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_run_single_benchmark FAILED [ 51%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_benchmark_calculation_accuracy FAILED [ 51%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_benchmark_results_by_type FAILED [ 52%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestIntegration::test_config_integration PASSED [ 52%]
tests\test_external_benchmarks.py::TestExternalBenchmarks::TestIntegration::test_prompt_system_integration FAILED [ 53%]
tests\test_external_benchmarks.py::TestExternalBenchmarksPerformance::test_suite_execution_performance ERROR [ 53%]
tests\test_external_benchmarks.py::TestExternalBenchmarksPerformance::test_memory_efficiency ERROR [ 54%]
tests\test_id_utilities.py::TestIdGeneration::test_generate_claim_id_format FAILED [ 54%]
tests\test_id_utilities.py::TestIdGeneration::test_generate_claim_id_uniqueness PASSED [ 54%]
tests\test_id_utilities.py::TestIdGeneration::test_generate_claim_id_timestamp_component PASSED [ 55%]
tests\test_id_utilities.py::TestIdGeneration::test_generate_claim_id_random_component PASSED [ 55%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_claim_id_valid PASSED [ 56%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_claim_id_invalid_format FAILED [ 56%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_claim_id_edge_cases FAILED [ 57%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_confidence_valid PASSED [ 57%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_confidence_invalid PASSED [ 57%]
tests\test_id_utilities.py::TestIdGeneration::test_validate_confidence_edge_cases PASSED [ 58%]
tests\test_id_utilities.py::TestIdGeneration::test_id_generation_performance PASSED [ 58%]
tests\test_id_utilities.py::TestIdGeneration::test_id_validation_performance PASSED [ 59%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_adapter_initialization PASSED [ 59%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_claim FAILED [ 60%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_duplicate_claim FAILED [ 60%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_claim FAILED [ 60%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_nonexistent_claim FAILED [ 61%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_update_claim FAILED [ 61%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_update_nonexistent_claim FAILED [ 62%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_list_claims ERROR [ 62%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_list_claims_with_filters ERROR [ 62%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_search_claims ERROR [ 63%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_vector_search ERROR [ 63%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_relationship FAILED [ 64%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_relationships FAILED [ 64%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_stats ERROR [ 65%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_claim_model_conversion PASSED [ 65%]
tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_factory_function PASSED [ 65%]
tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_invalid_claim_validation FAILED [ 66%]
tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_invalid_confidence FAILED [ 66%]
tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_large_number_of_claims FAILED [ 67%]
tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_concurrent_operations FAILED [ 67%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_claim_success FAILED [ 68%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_claim_validation_error FAILED [ 68%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_duplicate_claim FAILED [ 68%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claim_success FAILED [ 69%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claim_not_found FAILED [ 69%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_success FAILED [ 70%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_not_found FAILED [ 70%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_invalid_fields FAILED [ 71%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_delete_claim_success FAILED [ 71%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_search_claims FAILED [ 71%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_list_claims FAILED [ 72%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_list_claims_with_filters FAILED [ 72%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_vector_search FAILED [ 73%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claims_by_state FAILED [ 73%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_dirty_claims_management FAILED [ 74%]
tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_stats FAILED [ 74%]
tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_create_relationship_success FAILED [ 74%]
tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_create_relationship_nonexistent_claims FAILED [ 75%]
tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_relationships FAILED [ 75%]
tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_supporting_claims FAILED [ 76%]
tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_supported_claims FAILED [ 76%]
tests\test_lancedb_repositories.py::TestRepositoryIntegration::test_complex_claim_workflow FAILED [ 77%]
tests\test_lancedb_repositories.py::TestRepositoryIntegration::test_concurrent_operations FAILED [ 77%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_setup_logger_default PASSED [ 77%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_setup_logger_custom_level PASSED [ 78%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_setup_logger_with_handlers PASSED [ 78%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_get_logger_singleton PASSED [ 79%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_get_logger_different_names PASSED [ 79%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_logger_functionality PASSED [ 80%]
tests\test_monitoring_utilities.py::TestLoggingUtilities::test_logger_level_filtering FAILED [ 80%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_performance_monitor_creation FAILED [ 80%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_start_timing FAILED [ 81%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_end_timing FAILED [ 81%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_get_metrics_empty FAILED [ 82%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_get_metrics_with_operations FAILED [ 82%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_multiple_same_operations FAILED [ 82%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_performance_monitor_singleton PASSED [ 83%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_timing_without_start FAILED [ 83%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_concurrent_operations FAILED [ 84%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_metrics_reset FAILED [ 84%]
tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_performance_decorator FAILED [ 85%]
tests\test_process_layer.py::TestProcessLayerModels::test_instruction_type_enum PASSED [ 85%]
tests\test_process_layer.py::TestProcessLayerModels::test_processing_status_enum PASSED [ 85%]
tests\test_process_layer.py::TestProcessLayerModels::test_context_result_model FAILED [ 86%]
tests\test_process_layer.py::TestProcessLayerModels::test_instruction_model PASSED [ 86%]
tests\test_process_layer.py::TestProcessLayerModels::test_processing_result_model PASSED [ 87%]
tests\test_process_layer.py::TestProcessLayerModels::test_processing_config_model PASSED [ 87%]
tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_success FAILED [ 88%]
tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_claim_not_found FAILED [ 88%]
tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_with_hints FAILED [ 88%]
tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_depth_limit FAILED [ 89%]
tests\test_process_layer.py::TestProcessContextBuilder::test_context_caching FAILED [ 89%]
tests\test_process_layer.py::TestProcessContextBuilder::test_clear_cache FAILED [ 90%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_evaluate_claim_success FAILED [ 90%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_evaluate_claim_with_context FAILED [ 91%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_identify_instructions_success FAILED [ 91%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_identify_instructions_with_types FAILED [ 91%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_process_claim_complete_workflow FAILED [ 92%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_process_batch_claims FAILED [ 92%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_llm_bridge_error_handling FAILED [ 93%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_instruction_confidence_filtering FAILED [ 93%]
tests\test_process_layer.py::TestProcessLLMProcessor::test_processing_metrics FAILED [ 94%]
tests\test_process_layer.py::TestProcessLayerIntegration::test_complete_claim_processing_workflow FAILED [ 94%]
tests\test_process_layer.py::TestProcessLayerIntegration::test_batch_processing_with_relationships FAILED [ 94%]
tests\test_process_layer.py::TestProcessLayerIntegration::test_context_building_performance FAILED [ 95%]
tests\test_retry_utilities.py::TestRetryUtilities::test_enhanced_retry_config_defaults FAILED [ 95%]
tests\test_retry_utilities.py::TestRetryUtilities::test_enhanced_retry_config_custom PASSED [ 96%]
tests\test_retry_utilities.py::TestRetryUtilities::test_with_llm_retry_success_first_attempt PASSED [ 96%]
tests\test_retry_utilities.py::TestRetryUtilities::test_with_llm_retry_success_after_retries PASSED [ 97%]
tests\test_retry_utilities.py::TestRetryUtilities::test_with_llm_retry_max_attempts_exceeded FAILED [ 97%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_delay_calculation FAILED [ 97%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_with_jitter FAILED [ 98%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_max_delay_limit FAILED [ 98%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_specific_exception_types FAILED [ 99%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_config_validation FAILED [ 99%]
tests\test_retry_utilities.py::TestRetryUtilities::test_retry_function_preservation PASSED [100%]Running teardown with pytest sessionfinish...


=========================== ERRORS ============================
_ ERROR at setup of TestJudgePerformance.test_evaluation_performance _
file D:\projects\Conjecture\tests\test_enhanced_glm46_judge.py, line 404
      @pytest.mark.asyncio
      async def test_evaluation_performance(self, judge_config):
          """Test that evaluation completes in reasonable time"""
          import time

          judge = EnhancedGLM46Judge(judge_config)

          # Mock fast API response
          with patch.object(judge, '_call_glm46_judge') as mock_call:
              mock_call.return_value = '{"is_correct": true, "confidence": 80}'

              start_time = time.time()
              evaluation = await judge.evaluate_response(
                  "Test problem", "Test response", "Test expected", "general"
              )
              end_time = time.time()

              # Should complete quickly
              assert end_time - start_time < 2.0, f"Evaluation too slow: {end_time - start_time}s"
              assert isinstance(evaluation, JudgeEvaluation)
E       fixture 'judge_config' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_enhanced_glm46_judge.py:404
_ ERROR at setup of TestJudgePerformance.test_batch_evaluation_performance _
file D:\projects\Conjecture\tests\test_enhanced_glm46_judge.py, line 425
      @pytest.mark.asyncio
      async def test_batch_evaluation_performance(self, judge_config):
          """Test batch evaluation performance"""
          import time

          judge = EnhancedGLM46Judge(judge_config)

          # Mock API response
          with patch.object(judge, '_call_glm46_judge') as mock_call:
              mock_call.return_value = '{"is_correct": true, "confidence": 80}'

              # Create multiple test cases
              test_cases = [
                  ("Problem 1", "Response 1", "Expected 1", "mathematical"),
                  ("Problem 2", "Response 2", "Expected 2", "logical"),
                  ("Problem 3", "Response 3", "Expected 3", "general")
              ]

              start_time = time.time()
              evaluations = []
              for problem, response, expected, ptype in test_cases:
                  eval_result = await judge.evaluate_response(problem, response, expected, ptype)
                  evaluations.append(eval_result)
              end_time = time.time()

              # Should handle multiple evaluations efficiently
              assert end_time - start_time < 5.0, f"Batch evaluation too slow: {end_time - start_time}s"
              assert len(evaluations) == 3
              assert all(isinstance(e, JudgeEvaluation) for e in evaluations)
E       fixture 'judge_config' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_enhanced_glm46_judge.py:425
_ ERROR at setup of TestPromptSystemPerformance.test_response_time_performance _
file D:\projects\Conjecture\tests\test_enhanced_prompt_system.py, line 339
      @pytest.mark.asyncio
      async def test_response_time_performance(self, prompt_system):
          """Test that response times are reasonable"""
          import time

          problem = "What is 10 + 15?"
          start_time = time.time()

          response = await prompt_system.process_with_context(problem)

          end_time = time.time()
          response_time = end_time - start_time

          # Should complete quickly (under 1 second for processing)
          assert response_time < 1.0, f"Response too slow: {response_time}s"
          assert response is not None
E       fixture 'prompt_system' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_enhanced_prompt_system.py:339
_ ERROR at setup of TestPromptSystemPerformance.test_memory_efficiency _
file D:\projects\Conjecture\tests\test_enhanced_prompt_system.py, line 356
      @pytest.mark.asyncio
      async def test_memory_efficiency(self, prompt_system):
          """Test that memory usage is reasonable"""
          import gc
          import sys

          # Get initial memory usage
          gc.collect()
          initial_objects = len(gc.get_objects())

          # Process multiple problems
          problems = [
              "What is 2 + 2?",
              "Calculate 10 × 5",
              "Solve for x: x + 3 = 8"
          ]

          for problem in problems:
              await prompt_system.process_with_context(problem)

          # Check memory usage
          gc.collect()
          final_objects = len(gc.get_objects())
          object_increase = final_objects - initial_objects

          # Should not create excessive objects (arbitrary limit)
          assert object_increase < 1000, f"Too many objects created: {object_increase}"
E       fixture 'prompt_system' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_enhanced_prompt_system.py:356
_ ERROR at setup of TestExternalBenchmarksPerformance.test_suite_execution_performance _
file D:\projects\Conjecture\tests\test_external_benchmarks.py, line 472
      @pytest.mark.asyncio
      async def test_suite_execution_performance(self, benchmarks):
          """Test benchmark suite execution performance"""
          import time

          # Mock fast responses
          with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:
              mock_generate.return_value = "Fast response"

              with patch.object(benchmarks.prompt_system, 'process_with_context') as mock_conjecture:
                  mock_conjecture.return_value = Mock(response="Fast enhanced response", confidence=0.8)

                  start_time = time.time()
                  summary = await benchmarks.run_benchmark_suite("all", num_samples=1)
                  end_time = time.time()

                  # Should complete reasonably quickly even with mocking
                  assert end_time - start_time < 10.0, f"Suite too slow: {end_time - start_time}s"
                  assert summary["total_tasks"] == 5  # 5 benchmarks × 1 sample each
E       fixture 'benchmarks' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_external_benchmarks.py:472
_ ERROR at setup of TestExternalBenchmarksPerformance.test_memory_efficiency _
file D:\projects\Conjecture\tests\test_external_benchmarks.py, line 492
      @pytest.mark.asyncio
      async def test_memory_efficiency(self, benchmarks):
          """Test memory efficiency during benchmark execution"""
          import gc
          import sys

          # Get initial memory usage
          gc.collect()
          initial_objects = len(gc.get_objects())

          # Run benchmarks
          with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:
              mock_generate.return_value = "Memory test response"

              summary = await benchmarks.run_benchmark_suite("gsm8k", num_samples=2)

          # Check memory usage
          gc.collect()
          final_objects = len(gc.get_objects())
          object_increase = final_objects - initial_objects

          # Should not create excessive objects during execution
          assert object_increase < 2000, f"Too many objects created: {object_increase}"
          assert summary is not None
E       fixture 'benchmarks' not found
>       available fixtures: __pytest_repeat_step_number, _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_faker, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, event_loop_policy, faker, fast_localhost_mocks, fast_test_config, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, isolated_database, memory_monitor, mock_lmstudio_response, mock_ollama_response, mock_providers_config, mocker, module_mocker, monkeypatch, no_cover, optimized_pytest_config, package_mocker, performance_timer, pytestconfig, real_context_builder, real_data_manager, real_embedding_service, real_exploration_engine, real_vector_store, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_claim_data, sample_claims_data, scientific_test_validator, session_mocker, sqlite_manager, static_analysis_config, static_analysis_runner, temp_data_dir, temp_sqlite_db, test_config, test_data_factory, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, valid_claim, valid_relationship, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

D:\projects\Conjecture\tests\test_external_benchmarks.py:492
____ ERROR at setup of TestLanceDBAdapter.test_list_claims ____

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444DA86210>

    @pytest.fixture
    def sample_claims(self):
        """Create multiple sample claims"""
>       return [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about mathematics and logic",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
                tags=[f"tag{i}", "test"],
                state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE,
                embedding=[(i * 0.01)] * 384
            )
            for i in range(1, 11)
        ]

tests\test_lancedb_adapter.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x0000024450C91350>

    return [
>       Claim(
            id=f"c000000{i:02d}",
            content=f"Test claim {i} about mathematics and logic",
            confidence=0.7 + (i * 0.05),
            type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
            tags=[f"tag{i}", "test"],
            state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE,
            embedding=[(i * 0.01)] * 384
        )
        for i in range(1, 11)
    ]
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E   confidence
E     Input should be less than or equal to 1 [type=less_than_equal, input_value=1.05, input_type=float]
E       For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:63: ValidationError
_ ERROR at setup of TestLanceDBAdapter.test_list_claims_with_filters _

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3C3F50>

    @pytest.fixture
    def sample_claims(self):
        """Create multiple sample claims"""
>       return [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about mathematics and logic",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
                tags=[f"tag{i}", "test"],
                state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE,
                embedding=[(i * 0.01)] * 384
            )
            for i in range(1, 11)
        ]

tests\test_lancedb_adapter.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x0000024450C9D8F0>

    return [
>       Claim(
            id=f"c000000{i:02d}",
            content=f"Test claim {i} about mathematics and logic",
            confidence=0.7 + (i * 0.05),
            type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
            tags=[f"tag{i}", "test"],
            state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE,
            embedding=[(i * 0.01)] * 384
        )
        for i in range(1, 11)
    ]
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E   confidence
E     Input should be less than or equal to 1 [type=less_than_equal, input_value=1.05, input_type=float]
E       For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:63: ValidationError
___ ERROR at setup of TestLanceDBAdapter.test_search_claims ___

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3C1710>

    @pytest.fixture
    def sample_claims(self):
        """Create multiple sample claims"""
>       return [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about mathematics and logic",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
                tags=[f"tag{i}", "test"],
                state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE,
                embedding=[(i * 0.01)] * 384
            )
            for i in range(1, 11)
        ]

tests\test_lancedb_adapter.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x000002444F979D70>

    return [
>       Claim(
            id=f"c000000{i:02d}",
            content=f"Test claim {i} about mathematics and logic",
            confidence=0.7 + (i * 0.05),
            type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
            tags=[f"tag{i}", "test"],
            state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE,
            embedding=[(i * 0.01)] * 384
        )
        for i in range(1, 11)
    ]
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E   confidence
E     Input should be less than or equal to 1 [type=less_than_equal, input_value=1.05, input_type=float]
E       For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:63: ValidationError
___ ERROR at setup of TestLanceDBAdapter.test_vector_search ___

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3C1090>

    @pytest.fixture
    def sample_claims(self):
        """Create multiple sample claims"""
>       return [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about mathematics and logic",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
                tags=[f"tag{i}", "test"],
                state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE,
                embedding=[(i * 0.01)] * 384
            )
            for i in range(1, 11)
        ]

tests\test_lancedb_adapter.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x000002444FA1F650>

    return [
>       Claim(
            id=f"c000000{i:02d}",
            content=f"Test claim {i} about mathematics and logic",
            confidence=0.7 + (i * 0.05),
            type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
            tags=[f"tag{i}", "test"],
            state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE,
            embedding=[(i * 0.01)] * 384
        )
        for i in range(1, 11)
    ]
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E   confidence
E     Input should be less than or equal to 1 [type=less_than_equal, input_value=1.05, input_type=float]
E       For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:63: ValidationError
_____ ERROR at setup of TestLanceDBAdapter.test_get_stats _____

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3D1D90>

    @pytest.fixture
    def sample_claims(self):
        """Create multiple sample claims"""
>       return [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about mathematics and logic",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
                tags=[f"tag{i}", "test"],
                state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE,
                embedding=[(i * 0.01)] * 384
            )
            for i in range(1, 11)
        ]

tests\test_lancedb_adapter.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <range_iterator object at 0x000002444F9E4C10>

    return [
>       Claim(
            id=f"c000000{i:02d}",
            content=f"Test claim {i} about mathematics and logic",
            confidence=0.7 + (i * 0.05),
            type=[ClaimType.THESIS if i % 2 == 0 else ClaimType.CONCEPT],
            tags=[f"tag{i}", "test"],
            state=ClaimState.VALIDATED if i > 5 else ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE,
            embedding=[(i * 0.01)] * 384
        )
        for i in range(1, 11)
    ]
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E   confidence
E     Input should be less than or equal to 1 [type=less_than_equal, input_value=1.05, input_type=float]
E       For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:63: ValidationError
========================== FAILURES ===========================
__ TestClaimRelationships.test_self_relationship_prevention ___

self = <test_claim_relationships.TestClaimRelationships object at 0x000002444D304450>

    def test_self_relationship_prevention(self):
        """Test that claims don't reference themselves"""
>       claim = Claim(
            id="self_ref",
            content="Self-referencing claim",
            confidence=0.5,
            supports=["self_ref"],  # This would be invalid
            supported_by=["self_ref"]  # This would also be invalid
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E         Value error, Claim cannot support itself [type=value_error, input_value={'id': 'self_ref', 'conte...orted_by': ['self_ref']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/value_error

tests\test_claim_relationships.py:197: ValidationError
__ TestClaimLifecycleE2EFixed.test_batch_processing_workflow __

self = <test_e2e_claim_lifecycle_fixed.TestClaimLifecycleE2EFixed object at 0x000002444D322190>
db_manager = <src.data.optimized_sqlite_manager.OptimizedSQLiteManager object at 0x000002444F77DDD0>

    @pytest.mark.asyncio
    async def test_batch_processing_workflow(self, db_manager):
        """Test batch processing of multiple claims"""
    
        # Create multiple related claims
        claims_batch = [
            Claim(
                id="batch_1",
                content="Machine learning requires large datasets",
                confidence=0.6,
                state=ClaimState.EXPLORE,
                tags=["ml", "data"]
            ),
            Claim(
                id="batch_2",
                content="Deep learning is a subset of machine learning",
                confidence=0.8,
                state=ClaimState.VALIDATED,
                supports=["batch_1"],
                tags=["dl", "ml"]
            ),
            Claim(
                id="batch_3",
                content="Neural networks are the foundation of deep learning",
                confidence=0.9,
                state=ClaimState.VALIDATED,
                supports=["batch_2"],
                tags=["neural", "dl"]
            )
        ]
    
        # Add batch using batch_create_claims
>       claim_ids = await db_manager.batch_create_claims(claims_batch)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'OptimizedSQLiteManager' object has no attribute 'batch_create_claims'

tests\test_e2e_claim_lifecycle_fixed.py:252: AttributeError
-------------------- Captured stderr setup --------------------
INFO:src.data.connection_pool:Connection pool initialized with 1 connections
INFO:src.data.optimized_sqlite_manager:Optimized SQLite manager initialized with pool size: 6
--------------------- Captured log setup ----------------------
INFO     src.data.connection_pool:connection_pool.py:131 Connection pool initialized with 1 connections
INFO     src.data.optimized_sqlite_manager:optimized_sqlite_manager.py:124 Optimized SQLite manager initialized with pool size: 6
------------------ Captured stderr teardown -------------------
INFO:src.data.connection_pool:Connection pool closed
-------------------- Captured log teardown --------------------
INFO     src.data.connection_pool:connection_pool.py:149 Connection pool closed
_ TestConfigurationDrivenProcessingE2E.test_minimal_configuration_processing _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D32C250>
minimal_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F5C2AD0>

    @pytest.mark.asyncio
    async def test_minimal_configuration_processing(self, minimal_config):
        """Test processing with minimal configuration"""
    
        conjecture = Conjecture(config=minimal_config)
    
        # Verify configuration values
        assert conjecture.config.confidence_threshold == 0.85
        assert conjecture.config.confident_threshold == 0.5
        assert conjecture.config.max_context_size == 2000
        assert conjecture.config.batch_size == 5
        assert conjecture.config.debug is False
    
        # Test basic claim processing with minimal config
        test_claim = Claim(
            id="minimal_test",
            content="Simple test claim for minimal configuration",
            confidence=0.6,
            state=ClaimState.EXPLORE,
            tags=["minimal", "config", "test"]
        )
    
        # Add claim
        result = conjecture.add_claim(test_claim)
        assert result.success is True
        assert result.processed_claims == 1
    
        # Retrieve claim
        retrieved = await conjecture.get_claim("minimal_test")
>       assert retrieved is not None
E       assert None is not None

tests\test_e2e_configuration_driven.py:165: AssertionError
-------------------- Captured stdout call ---------------------
[LLM] Initializing OpenAI-compatible providers...
[LLM] ollama processor initialized (priority: 1)
[LLM] lm_studio processor initialized (priority: 2)
[LLM] Warning: All health checks failed. Using ollama as primary
[LLM] Initialized 2 providers: ['ollama', 'lm_studio']
DEBUG: _add_claim_to_repository called with claim.id=minimal_test
DEBUG: _add_claim_to_repository result = id='minimal_test' content='Simple test claim for minimal configuration' confidence=0.6 state=<ClaimState.EXPLORE: 'Explore'> type=[<ClaimType.CONCEPT: 'concept'>] tags=['minimal', 'config', 'test'] scope=<ClaimScope.USER_WORKSPACE: 'user-{workspace}'> supports=[] supported_by=[] created=datetime.datetime(2025, 12, 15, 16, 3, 18, 731624) updated=datetime.datetime(2025, 12, 15, 16, 3, 18, 731624) embedding=None is_dirty=True dirty=True dirty_reason=None dirty_timestamp=None dirty_priority=0
-------------------- Captured stderr call ---------------------
INFO:src.monitoring.performance_monitor:Performance monitoring started
WARNING:src.processing.llm.error_handling:Attempt 1 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 10.15s...
WARNING:src.processing.llm.error_handling:Attempt 2 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 18.47s...
WARNING:src.processing.llm.error_handling:Attempt 3 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 41.82s...
WARNING:src.processing.llm.error_handling:Attempt 4 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 69.78s...
ERROR:src.processing.llm.error_handling:Max retry attempts (5) reached
ERROR:src.processing.llm.error_handling:LLM operation failed (generation): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions
WARNING:src.processing.llm.error_handling:Attempt 1 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F62FE50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 14.64s...
WARNING:src.processing.llm.error_handling:Attempt 2 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C5D50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 29.75s...
WARNING:src.processing.llm.error_handling:Attempt 3 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C59D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 50.16s...
WARNING:src.processing.llm.error_handling:Attempt 4 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C6D50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 122.22s...
ERROR:src.processing.llm.error_handling:Max retry attempts (5) reached
ERROR:src.processing.llm.error_handling:LLM operation failed (generation): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F62E250>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
INFO:src.processing.enhanced_llm_router:Initializing 4 providers...
INFO:src.processing.enhanced_llm_router:Initialized provider: glm-4.5-air (priority: 1)
INFO:src.processing.enhanced_llm_router:Initialized provider: glm-4.6 (priority: 99)
INFO:src.processing.enhanced_llm_router:Initialized provider: gpt-oss-20b (priority: 10)
INFO:src.processing.enhanced_llm_router:Initialized provider: ollama-qwen-thinking (priority: 5)
INFO:src.processing.enhanced_llm_router:Successfully initialized 4 providers
INFO:src.processing.enhanced_llm_router:Health monitoring started
INFO:src.data.connection_pool:Connection pool initialized with 1 connections
INFO:src.data.optimized_sqlite_manager:Optimized SQLite manager initialized with pool size: 6
---------------------- Captured log call ----------------------
INFO     src.monitoring.performance_monitor:performance_monitor.py:106 Performance monitoring started
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 1 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 10.15s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 2 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 18.47s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 3 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 41.82s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 4 failed (ErrorType.UNKNOWN_ERROR): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions. Retrying in 69.78s...
ERROR    src.processing.llm.error_handling:error_handling.py:127 Max retry attempts (5) reached
ERROR    src.processing.llm.error_handling:error_handling.py:246 LLM operation failed (generation): 404 Client Error: Not Found for url: http://localhost:11434/v1/chat/completions
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 1 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F62FE50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 14.64s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 2 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C5D50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 29.75s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 3 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C59D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 50.16s...
WARNING  src.processing.llm.error_handling:error_handling.py:132 Attempt 4 failed (ErrorType.NETWORK_ERROR): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F5C6D50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 122.22s...
ERROR    src.processing.llm.error_handling:error_handling.py:127 Max retry attempts (5) reached
ERROR    src.processing.llm.error_handling:error_handling.py:246 LLM operation failed (generation): HTTPConnectionPool(host='localhost', port=1234): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002444F62E250>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:172 Initializing 4 providers...
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:204 Initialized provider: glm-4.5-air (priority: 1)
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:204 Initialized provider: glm-4.6 (priority: 99)
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:204 Initialized provider: gpt-oss-20b (priority: 10)
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:204 Initialized provider: ollama-qwen-thinking (priority: 5)
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:212 Successfully initialized 4 providers
INFO     src.processing.enhanced_llm_router:enhanced_llm_router.py:225 Health monitoring started
INFO     src.data.connection_pool:connection_pool.py:131 Connection pool initialized with 1 connections
INFO     src.data.optimized_sqlite_manager:optimized_sqlite_manager.py:124 Optimized SQLite manager initialized with pool size: 6
------------------ Captured stderr teardown -------------------
ERROR:src.processing.llm.error_handling:Non-retryable error (ErrorType.AUTHENTICATION_ERROR): 401 Client Error: Unauthorized for url: https://api.z.ai/api/coding/paas/v4/chat/completions
ERROR:src.processing.llm.error_handling:LLM operation failed (generation): 401 Client Error: Unauthorized for url: https://api.z.ai/api/coding/paas/v4/chat/completions
-------------------- Captured log teardown --------------------
ERROR    src.processing.llm.error_handling:error_handling.py:122 Non-retryable error (ErrorType.AUTHENTICATION_ERROR): 401 Client Error: Unauthorized for url: https://api.z.ai/api/coding/paas/v4/chat/completions
ERROR    src.processing.llm.error_handling:error_handling.py:246 LLM operation failed (generation): 401 Client Error: Unauthorized for url: https://api.z.ai/api/coding/paas/v4/chat/completions
_ TestConfigurationDrivenProcessingE2E.test_performance_configuration_processing _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D32CAD0>
performance_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F792E10>

    def test_performance_configuration_processing(self, performance_config):
        """Test processing with performance-optimized configuration"""
    
        conjecture = Conjecture(config=performance_config)
    
        # Verify performance configuration values
        assert conjecture.config.confidence_threshold == 0.95
        assert conjecture.config.confident_threshold == 0.7
        assert conjecture.config.max_context_size == 16000
        assert conjecture.config.batch_size == 50
    
        # Test batch processing with larger batches
        large_batch = []
        for i in range(25):  # Half of batch size
            claim = Claim(
                id=f"perf_test_{i}",
                content=f"Performance test claim {i} for batch processing",
                confidence=0.6 + (i * 0.01),  # Varying confidence
                state=ClaimState.EXPLORE,
                tags=["performance", "batch", f"tag_{i % 5}"]
            )
            large_batch.append(claim)
    
        # Add batch
        batch_result = conjecture.add_claims_batch(large_batch)
        assert batch_result.success is True
>       assert batch_result.processed_claims == 25
E       AssertionError: assert 50 == 25
E        +  where 50 = BatchResult(results=[ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated...datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754)), ProcessingResult(success=True, operation_type='add_claim', processed_items=1, updated_items=0, errors=[], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=datetime.datetime(2025, 12, 15, 16, 3, 21, 601754))], total_items=50, successful_operations=50, failed_operations=0).processed_claims

tests\test_e2e_configuration_driven.py:195: AssertionError
-------------------- Captured stderr call ---------------------
INFO:src.conjecture:Created claim c65814601 with confidence 0.6
INFO:src.conjecture:Created claim c65814601 with confidence 0.61
INFO:src.conjecture:Created claim c65814601 with confidence 0.62
INFO:src.conjecture:Created claim c65814601 with confidence 0.63
INFO:src.conjecture:Created claim c65814601 with confidence 0.64
INFO:src.conjecture:Created claim c65814601 with confidence 0.65
INFO:src.conjecture:Created claim c65814601 with confidence 0.6599999999999999
INFO:src.conjecture:Created claim c65814601 with confidence 0.6699999999999999
INFO:src.conjecture:Created claim c65814601 with confidence 0.6799999999999999
INFO:src.conjecture:Created claim c65814601 with confidence 0.69
INFO:src.conjecture:Created claim c65814601 with confidence 0.7
INFO:src.conjecture:Created claim c65814601 with confidence 0.71
INFO:src.conjecture:Created claim c65814601 with confidence 0.72
INFO:src.conjecture:Created claim c65814601 with confidence 0.73
INFO:src.conjecture:Created claim c65814601 with confidence 0.74
INFO:src.conjecture:Created claim c65814601 with confidence 0.75
INFO:src.conjecture:Created claim c65814601 with confidence 0.76
INFO:src.conjecture:Created claim c65814601 with confidence 0.77
INFO:src.conjecture:Created claim c65814601 with confidence 0.78
INFO:src.conjecture:Created claim c65814601 with confidence 0.79
INFO:src.conjecture:Created claim c65814601 with confidence 0.8
INFO:src.conjecture:Created claim c65814601 with confidence 0.8099999999999999
INFO:src.conjecture:Created claim c65814601 with confidence 0.82
INFO:src.conjecture:Created claim c65814601 with confidence 0.83
INFO:src.conjecture:Created claim c65814601 with confidence 0.84
---------------------- Captured log call ----------------------
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.6
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.61
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.62
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.63
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.64
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.65
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.6599999999999999
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.6699999999999999
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.6799999999999999
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.69
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.7
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.71
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.72
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.73
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.74
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.75
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.76
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.77
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.78
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.79
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.8
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.8099999999999999
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.82
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.83
INFO     src.conjecture:conjecture.py:177 Created claim c65814601 with confidence 0.84
_ TestConfigurationDrivenProcessingE2E.test_local_providers_configuration _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D321150>
local_providers_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F58BD50>

    def test_local_providers_configuration(self, local_providers_config):
        """Test processing with local providers configuration"""
    
        conjecture = Conjecture(config=local_providers_config)
    
        # Verify providers configuration
        assert len(conjecture.config.providers) == 2
    
>       provider_names = [p.name for p in conjecture.config.providers]
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_e2e_configuration_driven.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x000002444F7A14E0>

>   provider_names = [p.name for p in conjecture.config.providers]
                      ^^^^^^
E   AttributeError: 'dict' object has no attribute 'name'

tests\test_e2e_configuration_driven.py:214: AttributeError
-------------------- Captured stdout setup --------------------
Error loading config: 2 validation errors for ProviderConfig
url
  Field required [type=missing, input_value={'name': 'local-ollama', 'priority': 1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
model
  Field required [type=missing, input_value={'name': 'local-ollama', 'priority': 1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
Creating default configuration...
_ TestConfigurationDrivenProcessingE2E.test_provider_fallback_and_error_handling _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D32CF90>
local_providers_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F829FD0>

    def test_provider_fallback_and_error_handling(self, local_providers_config):
        """Test provider fallback mechanisms and error handling"""
    
        conjecture = Conjecture(config=local_providers_config)
    
        # Test with unavailable primary provider
        # This would normally test connection failures, but we'll simulate
    
        test_claim = Claim(
            id="fallback_test",
            content="Test claim for provider fallback",
            confidence=0.3,
            state=ClaimState.EXPLORE,
            tags=["fallback", "error", "test"]
        )
    
        # Add claim
        result = conjecture.add_claim(test_claim)
>       assert result.success is True
E       assert False is True
E        +  where False = ProcessingResult(success=False, operation_type='add_claim', processed_items=0, updated_items=0, errors=["There is no current event loop in thread 'MainThread'."], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=None).success

tests\test_e2e_configuration_driven.py:307: AssertionError
-------------------- Captured stdout setup --------------------
Error loading config: 2 validation errors for ProviderConfig
url
  Field required [type=missing, input_value={'name': 'local-ollama', 'priority': 1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
model
  Field required [type=missing, input_value={'name': 'local-ollama', 'priority': 1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
Creating default configuration...
_ TestConfigurationDrivenProcessingE2E.test_configuration_driven_batch_sizes _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D32D310>
minimal_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F852A10>
performance_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F853350>

    def test_configuration_driven_batch_sizes(self, minimal_config, performance_config):
        """Test different batch sizes based on configuration"""
    
        minimal_conjecture = Conjecture(config=minimal_config)
        performance_conjecture = Conjecture(config=performance_config)
    
        # Test minimal config batch size (5)
        minimal_batch = []
        for i in range(7):  # More than batch size
            claim = Claim(
                id=f"minimal_batch_{i}",
                content=f"Minimal batch test claim {i}",
                confidence=0.6,
                state=ClaimState.EXPLORE,
                tags=["minimal", "batch"]
            )
            minimal_batch.append(claim)
    
        # Should process in multiple batches due to batch size limit
        minimal_result = minimal_conjecture.add_claims_batch(minimal_batch)
>       assert minimal_result.success is True
E       assert False is True
E        +  where False = BatchResult(results=[], total_items=7, successful_operations=0, failed_operations=7).success

tests\test_e2e_configuration_driven.py:360: AssertionError
_ TestConfigurationDrivenProcessingE2E.test_configuration_memory_and_disk_usage _

self = <test_e2e_configuration_driven.TestConfigurationDrivenProcessingE2E object at 0x000002444D32D6D0>
performance_config = <src.config.unified_config.UnifiedConfig object at 0x000002444F8239D0>

    def test_configuration_memory_and_disk_usage(self, performance_config):
        """Test configuration effects on memory and disk usage"""
    
        conjecture = Conjecture(config=performance_config)
    
        # Add many claims to test memory usage
        memory_test_claims = []
        for i in range(100):
>           claim = Claim(
                id=f"memory_test_{i}",
                content=f"Memory usage test claim {i} with substantial content to test memory management",
                confidence=0.5 + (i % 50) * 0.01,  # Varying confidence
                state=ClaimState.EXPLORE,
                tags=["memory", "test", f"category_{i % 10}"],
                # Add some metadata to increase memory usage
                scope="user_workspace" if i % 2 == 0 else "team_workspace"
            )
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E           scope
E             Input should be 'user-{workspace}', 'team-{workspace}', 'team-wide' or 'public' [type=enum, input_value='user_workspace', input_type=str]

tests\test_e2e_configuration_driven.py:391: ValidationError
___ TestMultiClaimReasoningE2E.test_claim_network_reasoning ___

self = <test_e2e_multiclaim_reasoning.TestMultiClaimReasoningE2E object at 0x000002444D32F190>
conjecture_instance = <src.conjecture.Conjecture object at 0x000002444F666E10>

    def test_claim_network_reasoning(self, conjecture_instance):
        """Test reasoning across a network of related claims"""
    
        # Create a claim network representing a reasoning chain
        reasoning_network = [
            # Root claims (foundational knowledge)
            Claim(
                id="root_evidence_1",
                content="The Earth's climate has warmed by approximately 1.1°C since pre-industrial times",
                confidence=0.95,
                state=ClaimState.VALIDATED,
                tags=["climate", "evidence", "temperature"],
                supports=["intermediate_conclusion_1", "intermediate_conclusion_2"]
            ),
            Claim(
                id="root_evidence_2",
                content="Atmospheric CO2 concentrations have increased from 280ppm to over 415ppm since 1850",
                confidence=0.98,
                state=ClaimState.VALIDATED,
                tags=["climate", "co2", "evidence"],
                supports=["intermediate_conclusion_1"]
            ),
    
            # Intermediate conclusions
            Claim(
                id="intermediate_conclusion_1",
                content="Greenhouse gas emissions are the primary driver of recent climate change",
                confidence=0.85,
                state=ClaimState.EXPLORE,
                supported_by=["root_evidence_1", "root_evidence_2"],
                supports=["final_conclusion_1"],
                tags=["climate", "greenhouse", "causation"]
            ),
            Claim(
                id="intermediate_conclusion_2",
                content="Climate change is accelerating in recent decades",
                confidence=0.8,
                state=ClaimState.EXPLORE,
                supported_by=["root_evidence_1"],
                supports=["final_conclusion_1"],
                tags=["climate", "acceleration", "trend"]
            ),
    
            # Final conclusion
            Claim(
                id="final_conclusion_1",
                content="Urgent action is needed to reduce greenhouse gas emissions",
                confidence=0.4,
                state=ClaimState.EXPLORE,
                supported_by=["intermediate_conclusion_1", "intermediate_conclusion_2"],
                tags=["climate", "policy", "action"]
            )
        ]
    
        # Step 1: Add entire network
        batch_result = conjecture_instance.add_claims_batch(reasoning_network)
>       assert batch_result.success is True
E       assert False is True
E        +  where False = BatchResult(results=[], total_items=5, successful_operations=0, failed_operations=5).success

tests\test_e2e_multiclaim_reasoning.py:107: AssertionError
___ TestMultiClaimReasoningE2E.test_confidence_propagation ____

self = <test_e2e_multiclaim_reasoning.TestMultiClaimReasoningE2E object at 0x000002444D32F810>
conjecture_instance = <src.conjecture.Conjecture object at 0x000002444F82FCD0>

    def test_confidence_propagation(self, conjecture_instance):
        """Test confidence propagation through claim hierarchy"""
    
        # Create hierarchical claim structure
        hierarchy = [
            # Level 0: Base evidence (high confidence)
            Claim(
                id="base_1",
                content="Water boils at 100°C at sea level",
                confidence=0.99,
                state=ClaimState.VALIDATED,
                tags=["physics", "water", "boiling"],
                supports=["level1_1"]
            ),
            Claim(
                id="base_2",
                content="Salt raises the boiling point of water",
                confidence=0.95,
                state=ClaimState.VALIDATED,
                tags=["chemistry", "salt", "boiling"],
                supports=["level1_1"]
            ),
    
            # Level 1: Intermediate claim (medium confidence)
            Claim(
                id="level1_1",
                content="Salt water boils at a higher temperature than pure water",
                confidence=0.7,
                state=ClaimState.EXPLORE,
                supported_by=["base_1", "base_2"],
                supports=["level2_1"],
                tags=["physics", "chemistry", "solution"]
            ),
    
            # Level 2: Derived claim (low confidence initially)
            Claim(
                id="level2_1",
                content="Adding salt to water allows it to reach higher temperatures before boiling",
                confidence=0.4,
                state=ClaimState.EXPLORE,
                supported_by=["level1_1"],
                tags=["cooking", "practical", "application"]
            )
        ]
    
        # Add hierarchy
        for claim in hierarchy:
            result = conjecture_instance.add_claim(claim)
>           assert result.success is True
E           assert False is True
E            +  where False = ProcessingResult(success=False, operation_type='add_claim', processed_items=0, updated_items=0, errors=["There is no current event loop in thread 'MainThread'."], warnings=[], execution_time=None, tokens_used=None, result_metadata={}, message='', started_at=None, completed_at=None).success

tests\test_e2e_multiclaim_reasoning.py:243: AssertionError
__ TestMultiClaimReasoningE2E.test_batch_reasoning_workflow ___

self = <test_e2e_multiclaim_reasoning.TestMultiClaimReasoningE2E object at 0x000002444D32FE90>
conjecture_instance = <src.conjecture.Conjecture object at 0x000002444F8A99D0>

    def test_batch_reasoning_workflow(self, conjecture_instance):
        """Test batch processing of reasoning workflow"""
    
        # Create diverse claim set for batch processing
        diverse_claims = [
            # Scientific claims
            Claim(
                id="science_1",
                content="DNA replication follows a semi-conservative mechanism",
                confidence=0.9,
                state=ClaimState.VALIDATED,
                tags=["biology", "dna", "replication"]
            ),
            Claim(
                id="science_2",
                content="CRISPR-Cas9 can edit DNA with high precision",
                confidence=0.85,
                state=ClaimState.EXPLORE,
                supported_by=["science_1"],
                tags=["biology", "crispr", "editing"]
            ),
    
            # Technology claims
            Claim(
                id="tech_1",
                content="Quantum computers can solve certain problems exponentially faster",
                confidence=0.7,
                state=ClaimState.EXPLORE,
                tags=["quantum", "computing", "complexity"]
            ),
    
            # Mathematical claims
            Claim(
                id="math_1",
                content="The Riemann hypothesis remains unproven",
                confidence=0.99,
                state=ClaimState.VALIDATED,
                tags=["mathematics", "riemann", "hypothesis"]
            ),
            Claim(
                id="math_2",
                content="Prime number distribution follows logarithmic patterns",
                confidence=0.95,
                state=ClaimState.VALIDATED,
                supported_by=["math_1"],
                supports=["science_1"],  # Cross-domain support
                tags=["mathematics", "primes", "distribution"]
            )
        ]
    
        # Step 1: Batch add
        batch_result = conjecture_instance.add_claims_batch(diverse_claims)
>       assert batch_result.success is True
E       assert False is True
E        +  where False = BatchResult(results=[], total_items=5, successful_operations=0, failed_operations=5).success

tests\test_e2e_multiclaim_reasoning.py:356: AssertionError
_ TestEnhancedGLM46Judge.TestFallbackEvaluation.test_fallback_evaluation_correct _

self = <test_enhanced_glm46_judge.TestEnhancedGLM46Judge.TestFallbackEvaluation object at 0x000002444D398990>
judge_config = {'key': 'test_api_key', 'model': 'glm-4.6', 'url': 'http://test-api.com'}

    def test_fallback_evaluation_correct(self, judge_config):
        """Test fallback evaluation for correct responses"""
        judge = EnhancedGLM46Judge(judge_config)
        from datetime import datetime
        start_time = datetime.now()
    
        evaluation = judge._fallback_evaluation(
            problem="What is 2+2?",
            response="The answer is 4",
            expected="4",
            start_time=start_time
        )
    
        assert isinstance(evaluation, JudgeEvaluation)
>       assert evaluation.is_correct is True
E       AssertionError: assert False is True
E        +  where False = JudgeEvaluation(is_correct=False, confidence=40, reasoning_quality='fair', problem_type_match=True, enhancement_usage='minimal', feedback='Fallback evaluation - GLM-4.6 unavailable', detailed_scores={'correctness': 40, 'methodology': 50, 'clarity': 50, 'completeness': 50, 'enhancement_usage': 30}, evaluation_time=0.0).is_correct

tests\test_enhanced_glm46_judge.py:189: AssertionError
_ TestEnhancedGLM46Judge.TestErrorHandling.test_timeout_handling _

self = <test_enhanced_glm46_judge.TestEnhancedGLM46Judge.TestErrorHandling object at 0x000002444D39B250>
judge_config = {'key': 'test_api_key', 'model': 'glm-4.6', 'url': 'http://test-api.com'}

    @pytest.mark.asyncio
    async def test_timeout_handling(self, judge_config):
        """Test handling of timeout scenarios"""
        judge = EnhancedGLM46Judge(judge_config)
    
        # Mock timeout
        with patch('aiohttp.ClientSession.post') as mock_post:
            mock_post.side_effect = asyncio.TimeoutError("Request timeout")
    
            evaluation = await judge.evaluate_response(
                "Test problem", "Test response", "Test expected", "general"
            )
    
            # Should fall back gracefully
            assert isinstance(evaluation, JudgeEvaluation)
>           assert evaluation.is_correct is False  # Conservative fallback
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AssertionError: assert True is False
E            +  where True = JudgeEvaluation(is_correct=True, confidence=60, reasoning_quality='fair', problem_type_match=True, enhancement_usage='minimal', feedback='Fallback evaluation - GLM-4.6 unavailable', detailed_scores={'correctness': 60, 'methodology': 50, 'clarity': 50, 'completeness': 50, 'enhancement_usage': 30}, evaluation_time=0.001006).is_correct

tests\test_enhanced_glm46_judge.py:381: AssertionError
-------------------- Captured stderr call ---------------------
ERROR:src.benchmarking.enhanced_glm46_judge:GLM-4.6 evaluation failed: Request timeout
---------------------- Captured log call ----------------------
ERROR    src.benchmarking.enhanced_glm46_judge:enhanced_glm46_judge.py:96 GLM-4.6 evaluation failed: Request timeout
_ TestEnhancedPromptSystem.TestProblemTypeDetection.test_logical_detection _

self = <test_enhanced_prompt_system.TestEnhancedPromptSystem.TestProblemTypeDetection object at 0x000002444D3B5510>
prompt_system = <src.agent.prompt_system.PromptSystem object at 0x000002444F836E10>

    @pytest.mark.asyncio
    async def test_logical_detection(self, prompt_system):
        """Test logical problem detection"""
        problems = [
            "If all A are B and some B are C, what follows?",
            "Given P implies Q and Q implies R, does P imply R?",
            "All humans are mortal. Socrates is human. Therefore?"
        ]
    
        for problem in problems:
            problem_type = prompt_system._detect_problem_type(problem)
>           assert problem_type == ProblemType.LOGICAL, f"Failed to detect logical in: {problem}"
E           AssertionError: Failed to detect logical in: Given P implies Q and Q implies R, does P imply R?
E           assert <ProblemType.MATHEMATICAL: 'mathematical'> == <ProblemType.LOGICAL: 'logical'>
E            +  where <ProblemType.LOGICAL: 'logical'> = ProblemType.LOGICAL

tests\test_enhanced_prompt_system.py:53: AssertionError
_ TestEnhancedPromptSystem.TestProblemTypeDetection.test_scientific_detection _

self = <test_enhanced_prompt_system.TestEnhancedPromptSystem.TestProblemTypeDetection object at 0x000002444D39B750>
prompt_system = <src.agent.prompt_system.PromptSystem object at 0x000002444F8AC910>

    @pytest.mark.asyncio
    async def test_scientific_detection(self, prompt_system):
        """Test scientific problem detection"""
        problems = [
            "Design an experiment to test plant growth",
            "Analyze the data from this scientific observation",
            "Form a hypothesis about the experimental results"
        ]
    
        for problem in problems:
            problem_type = prompt_system._detect_problem_type(problem)
            # Note: Current implementation may detect as other types, this test documents current behavior
>           assert problem_type in [ProblemType.SCIENTIFIC, ProblemType.GENERAL], f"Unexpected detection: {problem_type}"
E           AssertionError: Unexpected detection: ProblemType.MATHEMATICAL
E           assert <ProblemType.MATHEMATICAL: 'mathematical'> in [<ProblemType.SCIENTIFIC: 'scientific'>, <ProblemType.GENERAL: 'general'>]

tests\test_enhanced_prompt_system.py:67: AssertionError
_ TestEnhancedPromptSystem.TestDifficultyEstimation.test_easy_detection _

self = <test_enhanced_prompt_system.TestEnhancedPromptSystem.TestDifficultyEstimation object at 0x000002444D3B59D0>
prompt_system = <src.agent.prompt_system.PromptSystem object at 0x000002444F821690>

    @pytest.mark.asyncio
    async def test_easy_detection(self, prompt_system):
        """Test easy problem detection"""
        easy_problems = [
            "What is 2 + 2?",
            "How many days in a week?",
            "What color is the sky?"
        ]
    
        for problem in easy_problems:
            difficulty = prompt_system._estimate_difficulty(problem)
>           assert difficulty == Difficulty.EASY, f"Failed to detect easy in: {problem}"
E           AssertionError: Failed to detect easy in: What color is the sky?
E           assert <Difficulty.MEDIUM: 'medium'> == <Difficulty.EASY: 'easy'>
E            +  where <Difficulty.EASY: 'easy'> = Difficulty.EASY

tests\test_enhanced_prompt_system.py:110: AssertionError
_ TestEnhancedPromptSystem.TestResponseParsing.test_mathematical_parsing _

self = <test_enhanced_prompt_system.TestEnhancedPromptSystem.TestResponseParsing object at 0x000002444D3B6B50>
response_parser = <src.agent.prompt_system.ResponseParser object at 0x000002444F811BD0>

    @pytest.mark.asyncio
    async def test_mathematical_parsing(self, response_parser):
        """Test mathematical response parsing"""
        response = "The answer is 42. I calculated this by multiplying 6 by 7."
        parsed = response_parser.parse_response(response, 'mathematical')
    
        assert parsed['answer'] is not None
        assert 'workings' in parsed
        assert 'confidence' in parsed
        assert 'numbers_found' in parsed
>       assert parsed['has_final_answer'] is True
E       assert False is True

tests\test_enhanced_prompt_system.py:296: AssertionError
_ TestExternalBenchmarks.TestGSM8KSamples.test_gsm8k_math_problems _

self = <test_external_benchmarks.TestExternalBenchmarks.TestGSM8KSamples object at 0x000002444D3F5CD0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FA56250>

    def test_gsm8k_math_problems(self, benchmarks):
        """Test GSM8K math problem content"""
>       samples = benchmarks.get_gmm8k_samples(num_samples=1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExternalBenchmarks' object has no attribute 'get_gmm8k_samples'

tests\test_external_benchmarks.py:112: AttributeError
_ TestExternalBenchmarks.TestGSM8KSamples.test_gsm8k_metadata_grade_level _

self = <test_external_benchmarks.TestExternalBenchmarks.TestGSM8KSamples object at 0x000002444D3F6090>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444F993210>

    def test_gsm8k_metadata_grade_level(self, benchmarks):
        """Test GSM8K grade level metadata"""
>       samples = benchmarks.get_gmm8k_samples(num_samples=1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ExternalBenchmarks' object has no attribute 'get_gmm8k_samples'

tests\test_external_benchmarks.py:124: AttributeError
_ TestExternalBenchmarks.TestTaskEvaluation.test_evaluate_task_structure _

self = <test_external_benchmarks.TestExternalBenchmarks.TestTaskEvaluation object at 0x000002444D3F8C90>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FA87650>

    @pytest.mark.asyncio
    async def test_evaluate_task_structure(self, benchmarks):
        """Test task evaluation returns proper structure"""
        task = ExternalBenchmarkTask(
            task_id="test_001",
            benchmark_name="Test",
            domain="test",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        # Mock the LLM bridge
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:221: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FA85E10>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FA85E10>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FA85A90> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestTaskEvaluation.test_evaluate_task_answer_extraction _

self = <test_external_benchmarks.TestExternalBenchmarks.TestTaskEvaluation object at 0x000002444D3F94D0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FA3CB10>

    @pytest.mark.asyncio
    async def test_evaluate_task_answer_extraction(self, benchmarks):
        """Test answer extraction from responses"""
        task = ExternalBenchmarkTask(
            task_id="test_002",
            benchmark_name="Test",
            domain="test",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        # Test multiple choice answer extraction
        test_cases = [
            ("The answer is B. 4", "4"),
            ("I choose B: 4", "4"),
            ("Answer: 4", "4"),
            ("4", "4"),
            ("The correct answer is the second option, which is 4", "4")
        ]
    
        for response_text in test_cases:
>           with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:258: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FA3DED0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FA3DED0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FA3E050> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestTaskEvaluation.test_evaluate_task_confidence_calculation _

self = <test_external_benchmarks.TestExternalBenchmarks.TestTaskEvaluation object at 0x000002444D3F9D10>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444F977410>

    @pytest.mark.asyncio
    async def test_evaluate_task_confidence_calculation(self, benchmarks):
        """Test confidence calculation"""
        task = ExternalBenchmarkTask(
            task_id="test_003",
            benchmark_name="Test",
            domain="test",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        confidence_indicators = [
            ("definitely correct", 0.9),
            ("probably correct", 0.7),
            ("maybe correct", 0.5),
            ("uncertain", 0.3)
        ]
    
        for indicator, expected_confidence in confidence_indicators:
>           with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444F9D4410>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444F9D4410>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444F9D4CD0> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestTaskEvaluation.test_evaluate_task_correctness_evaluation _

self = <test_external_benchmarks.TestExternalBenchmarks.TestTaskEvaluation object at 0x000002444D3FA590>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FAD3B90>

    @pytest.mark.asyncio
    async def test_evaluate_task_correctness_evaluation(self, benchmarks):
        """Test correctness evaluation"""
        task = ExternalBenchmarkTask(
            task_id="test_004",
            benchmark_name="Test",
            domain="test",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        test_cases = [
            ("4", True),  # Exact match
            ("The answer is 4", True),  # Contains match
            ("5", False),  # Wrong answer
            ("Three", False)  # Wrong answer
        ]
    
        for response_text, expected_correct in test_cases:
>           with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAD1250>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAD1250>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FAD0A10> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestTaskEvaluation.test_evaluate_task_error_handling _

self = <test_external_benchmarks.TestExternalBenchmarks.TestTaskEvaluation object at 0x000002444D3FADD0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FBA99D0>

    @pytest.mark.asyncio
    async def test_evaluate_task_error_handling(self, benchmarks):
        """Test error handling in task evaluation"""
        task = ExternalBenchmarkTask(
            task_id="test_005",
            benchmark_name="Test",
            domain="test",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        # Mock LLM bridge error
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:329: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FBABCD0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FBABCD0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FBAABD0> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestBenchmarkSuite.test_run_all_benchmarks_structure _

self = <test_external_benchmarks.TestExternalBenchmarks.TestBenchmarkSuite object at 0x000002444D3FB6D0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FAD3A10>

    @pytest.mark.asyncio
    async def test_run_all_benchmarks_structure(self, benchmarks):
        """Test all benchmarks execution structure"""
        # Mock LLM responses
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:347: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAFD290>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAFD290>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FAFFCD0> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestBenchmarkSuite.test_run_single_benchmark _

self = <test_external_benchmarks.TestExternalBenchmarks.TestBenchmarkSuite object at 0x000002444D3FBF10>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FB99650>

    @pytest.mark.asyncio
    async def test_run_single_benchmark(self, benchmarks):
        """Test running single benchmark"""
        # Mock LLM responses
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:376: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FB9A1D0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FB9A1D0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FB99E50> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestBenchmarkSuite.test_benchmark_calculation_accuracy _

self = <test_external_benchmarks.TestExternalBenchmarks.TestBenchmarkSuite object at 0x000002444D3FB5D0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FAC8CD0>

    @pytest.mark.asyncio
    async def test_benchmark_calculation_accuracy(self, benchmarks):
        """Test benchmark calculation accuracy"""
        # Mock different accuracy levels
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAC9E90>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FAC9E90>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FAC98D0> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestBenchmarkSuite.test_benchmark_results_by_type _

self = <test_external_benchmarks.TestExternalBenchmarks.TestBenchmarkSuite object at 0x000002444D3FB1D0>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x000002444FB38750>

    @pytest.mark.asyncio
    async def test_benchmark_results_by_type(self, benchmarks):
        """Test benchmark results grouped by benchmark type"""
>       with patch.object(benchmarks.llm_bridge, 'generate') as mock_generate:

tests\test_external_benchmarks.py:409: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FB396D0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()
                          ^^^^^^^^^^^^^^^^^^^

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x000002444FB396D0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <src.processing.llm_bridge.LLMBridge object at 0x000002444FB39390> does not have the attribute 'generate'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\unittest\mock.py:1419: AttributeError
_ TestExternalBenchmarks.TestIntegration.test_prompt_system_integration _

self = <test_external_benchmarks.TestExternalBenchmarks.TestIntegration object at 0x000002444D3F9C50>
benchmarks = <src.benchmarking.external_benchmarks.ExternalBenchmarks object at 0x0000024450D0A5D0>

    @pytest.mark.asyncio
    async def test_prompt_system_integration(self, benchmarks):
        """Test prompt system integration for conjecture evaluation"""
        task = ExternalBenchmarkTask(
            task_id="integration_test",
            benchmark_name="Test",
            domain="mathematical",
            question="What is 2+2?",
            choices=["3", "4", "5"],
            correct_answer="4"
        )
    
        # Mock enhanced prompt system response
        with patch.object(benchmarks.prompt_system, 'process_with_context') as mock_conjecture:
            mock_response = Mock()
            mock_response.response = "4"
            mock_response.confidence = 0.85
            mock_response.metadata = {
                'problem_type': 'mathematical',
                'enhancements_applied': 2,
                'enhancement_types': ['mathematical_reasoning', 'self_verification']
            }
            mock_conjecture.return_value = mock_response
    
            result = await benchmarks.evaluate_task(task, using_conjecture=True)
    
            assert result.using_conjecture is True
>           assert result.predicted_answer == "4"
E           AssertionError: assert '' == '4'
E             
E             - 4

tests\test_external_benchmarks.py:465: AssertionError
-------------------- Captured stderr call ---------------------
2025-12-15 11:03:24,609 - src.benchmarking.external_benchmarks - ERROR - Error evaluating task integration_test: expected string or bytes-like object, got 'Mock'
ERROR:src.benchmarking.external_benchmarks:Error evaluating task integration_test: expected string or bytes-like object, got 'Mock'
---------------------- Captured log call ----------------------
ERROR    src.benchmarking.external_benchmarks:external_benchmarks.py:207 Error evaluating task integration_test: expected string or bytes-like object, got 'Mock'
_______ TestIdGeneration.test_generate_claim_id_format ________

self = <test_id_utilities.TestIdGeneration object at 0x000002444D401BD0>

    def test_generate_claim_id_format(self):
        """Test generated claim ID follows expected format"""
        claim_id = generate_claim_id()
    
        # Should start with 'c'
        assert claim_id.startswith('c')
    
        # Should be a string
        assert isinstance(claim_id, str)
    
        # Should match pattern: c + timestamp + random
        pattern = r'^c\d{13,}$'
>       assert re.match(pattern, claim_id) is not None
E       AssertionError: assert None is not None
E        +  where None = <function match at 0x0000024388E098A0>('^c\\d{13,}$', 'c1765814604646_9cafadb7')
E        +    where <function match at 0x0000024388E098A0> = re.match

tests\test_id_utilities.py:28: AssertionError
___ TestIdGeneration.test_validate_claim_id_invalid_format ____

self = <test_id_utilities.TestIdGeneration object at 0x000002444D403A10>

    def test_validate_claim_id_invalid_format(self):
        """Test validation of invalid claim ID formats"""
        invalid_ids = [
            "",  # Empty
            "123",  # Missing 'c' prefix
            "x1234567890123",  # Wrong prefix
            "c123",  # Too short
            "c123456789012a",  # Contains letter
            "c-1234567890123",  # Contains dash
            "c 1234567890123",  # Contains space
            "C1234567890123",  # Uppercase
            "claim123",  # Word prefix
        ]
    
        for claim_id in invalid_ids:
>           assert validate_claim_id(claim_id) is False
E           AssertionError: assert True is False
E            +  where True = validate_claim_id('123')

tests\test_id_utilities.py:99: AssertionError
_____ TestIdGeneration.test_validate_claim_id_edge_cases ______

self = <test_id_utilities.TestIdGeneration object at 0x000002444D408050>

    def test_validate_claim_id_edge_cases(self):
        """Test validation edge cases"""
        # Non-string input
        try:
            assert validate_claim_id(123) is False
        except (TypeError, AttributeError):
            # Expected behavior for non-string input
            pass
    
        try:
            assert validate_claim_id([]) is False
        except (TypeError, AttributeError):
            # Expected behavior for non-string input
            pass
    
        try:
            assert validate_claim_id({}) is False
        except (TypeError, AttributeError):
            # Expected behavior for non-string input
            pass
    
        # Only 'c' prefix
>       assert validate_claim_id("c") is False
E       AssertionError: assert True is False
E        +  where True = validate_claim_id('c')

tests\test_id_utilities.py:123: AssertionError
____________ TestLanceDBAdapter.test_create_claim _____________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444DA85E90>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB5D40>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_create_claim(self, adapter, sample_claim):
        """Test claim creation"""
        # Create claim
>       result = await adapter.create_claim(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:99: AttributeError
_______ TestLanceDBAdapter.test_create_duplicate_claim ________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444D007110>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB6140>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_create_duplicate_claim(self, adapter, sample_claim):
        """Test creating duplicate claim should fail"""
        # Create claim first time
>       await adapter.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:110: AttributeError
______________ TestLanceDBAdapter.test_get_claim ______________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3C3B10>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB5B40>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_get_claim(self, adapter, sample_claim):
        """Test retrieving a claim"""
        # Create claim
>       await adapter.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:120: AttributeError
________ TestLanceDBAdapter.test_get_nonexistent_claim ________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3D0350>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB5F40>

    @pytest.mark.asyncio
    async def test_get_nonexistent_claim(self, adapter):
        """Test retrieving non-existent claim"""
>       retrieved = await adapter.get_claim("c9999999")
                          ^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'get_claim'

tests\test_lancedb_adapter.py:134: AttributeError
____________ TestLanceDBAdapter.test_update_claim _____________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444DB3F110>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB4E40>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_update_claim(self, adapter, sample_claim):
        """Test updating a claim"""
        # Create claim
>       await adapter.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:141: AttributeError
______ TestLanceDBAdapter.test_update_nonexistent_claim _______

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444D114F50>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444FAB5A40>

    @pytest.mark.asyncio
    async def test_update_nonexistent_claim(self, adapter):
        """Test updating non-existent claim"""
        with pytest.raises(ClaimNotFoundError):
>           await adapter.update_claim("c9999999", {"content": "Updated"})
                  ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'update_claim'

tests\test_lancedb_adapter.py:160: AttributeError
_________ TestLanceDBAdapter.test_create_relationship _________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3D06D0>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444F7AFA40>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_create_relationship(self, adapter, sample_claim):
        """Test creating a relationship"""
        # Create two claims
        claim1 = sample_claim
        claim2 = Claim(
            id="c0000002",
            content="Supporting claim for testing",
            confidence=0.9,
            type=[ClaimType.REFERENCE],
            tags=["support", "test"],
            state=ClaimState.VALIDATED,
            scope=ClaimScope.USER_WORKSPACE
        )
    
>       await adapter.create_claim(claim1)
              ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:246: AttributeError
__________ TestLanceDBAdapter.test_get_relationships __________

self = <test_lancedb_adapter.TestLanceDBAdapter object at 0x000002444F3D1590>
adapter = <async_generator object TestLanceDBAdapter.adapter at 0x000002444F7AF540>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_get_relationships(self, adapter, sample_claim):
        """Test retrieving relationships for a claim"""
        # Create two claims
        claim1 = sample_claim
        claim2 = Claim(
            id="c0000002",
            content="Supporting claim",
            confidence=0.9,
            type=[ClaimType.REFERENCE],
            tags=["support"],
            state=ClaimState.VALIDATED,
            scope=ClaimScope.USER_WORKSPACE
        )
    
>       await adapter.create_claim(claim1)
              ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:275: AttributeError
__ TestLanceDBAdapterEdgeCases.test_invalid_claim_validation __

self = <test_lancedb_adapter.TestLanceDBAdapterEdgeCases object at 0x000002444F3D3650>
adapter = <async_generator object TestLanceDBAdapterEdgeCases.adapter at 0x000002444F7ACA40>

    @pytest.mark.asyncio
    async def test_invalid_claim_validation(self, adapter):
        """Test claim validation during creation"""
        # Test short content
>       short_claim = Claim(
            id="c0000001",
            content="Short",  # Too short
            confidence=0.5,
            type=[ClaimType.CONCEPT],
            tags=["test"]
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E       content
E         String should have at least 10 characters [type=string_too_short, input_value='Short', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.5/v/string_too_short

tests\test_lancedb_adapter.py:357: ValidationError
_____ TestLanceDBAdapterEdgeCases.test_invalid_confidence _____

self = <test_lancedb_adapter.TestLanceDBAdapterEdgeCases object at 0x000002444F3D3E90>
adapter = <async_generator object TestLanceDBAdapterEdgeCases.adapter at 0x000002444F7AC440>

    @pytest.mark.asyncio
    async def test_invalid_confidence(self, adapter):
        """Test invalid confidence values"""
        # Test high confidence
>       high_claim = Claim(
            id="c0000001",
            content="Valid content for testing purposes",
            confidence=1.5,  # Too high
            type=[ClaimType.CONCEPT],
            tags=["test"]
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E       confidence
E         Input should be less than or equal to 1 [type=less_than_equal, input_value=1.5, input_type=float]
E           For further information visit https://errors.pydantic.dev/2.5/v/less_than_equal

tests\test_lancedb_adapter.py:372: ValidationError
___ TestLanceDBAdapterEdgeCases.test_large_number_of_claims ___

self = <test_lancedb_adapter.TestLanceDBAdapterEdgeCases object at 0x000002444F3D4710>
adapter = <async_generator object TestLanceDBAdapterEdgeCases.adapter at 0x000002444F7AD040>

    @pytest.mark.asyncio
    async def test_large_number_of_claims(self, adapter):
        """Test handling large numbers of claims"""
        claims = []
        for i in range(100):
            claim = Claim(
                id=f"c{i:08d}",
                content=f"Test claim number {i} with some content",
                confidence=0.5 + (i % 50) / 100,
                type=[ClaimType.CONCEPT],
                tags=[f"tag{i}", "bulk_test"],
                state=ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE
            )
            claims.append(claim)
    
        # Create all claims
        for claim in claims:
>           await adapter.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:405: AttributeError
___ TestLanceDBAdapterEdgeCases.test_concurrent_operations ____

self = <test_lancedb_adapter.TestLanceDBAdapterEdgeCases object at 0x000002444F3D4F90>
adapter = <async_generator object TestLanceDBAdapterEdgeCases.adapter at 0x000002444F7AD340>

    @pytest.mark.asyncio
    async def test_concurrent_operations(self, adapter):
        """Test concurrent claim operations"""
        async def create_claims(start_id: int, count: int):
            for i in range(count):
                claim = Claim(
                    id=f"c{start_id + i:08d}",
                    content=f"Concurrent claim {start_id + i}",
                    confidence=0.8,
                    type=[ClaimType.CONCEPT],
                    tags=["concurrent"],
                    state=ClaimState.EXPLORE,
                    scope=ClaimScope.USER_WORKSPACE
                )
                await adapter.create_claim(claim)
    
        # Run concurrent operations
        tasks = [
            create_claims(1000, 10),
            create_claims(2000, 10),
            create_claims(3000, 10)
        ]
    
>       await asyncio.gather(*tasks)

tests\test_lancedb_adapter.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

start_id = 1000, count = 10

    async def create_claims(start_id: int, count: int):
        for i in range(count):
            claim = Claim(
                id=f"c{start_id + i:08d}",
                content=f"Concurrent claim {start_id + i}",
                confidence=0.8,
                type=[ClaimType.CONCEPT],
                tags=["concurrent"],
                state=ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE
            )
>           await adapter.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_adapter.py:425: AttributeError
____ TestLanceDBClaimRepository.test_create_claim_success _____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3D46D0>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F783140>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_create_claim_success(self, repository, sample_claim):
        """Test successful claim creation"""
>       result = await repository.create_claim(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:80: AttributeError
_ TestLanceDBClaimRepository.test_create_claim_validation_error _

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3D6F50>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F7AE440>

    @pytest.mark.asyncio
    async def test_create_claim_validation_error(self, repository):
        """Test claim creation with validation error"""
        # Create invalid claim with short content
>       invalid_claim = Claim(
            id="c0000001",
            content="Short",  # Too short
            confidence=0.5,
            type=[ClaimType.CONCEPT],
            tags=["test"]
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for Claim
E       content
E         String should have at least 10 characters [type=string_too_short, input_value='Short', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.5/v/string_too_short

tests\test_lancedb_repositories.py:92: ValidationError
___ TestLanceDBClaimRepository.test_create_duplicate_claim ____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3D7750>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F781740>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_create_duplicate_claim(self, repository, sample_claim):
        """Test creating duplicate claim"""
        # Create claim first time
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:109: AttributeError
______ TestLanceDBClaimRepository.test_get_claim_success ______

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3D7F90>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F783640>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_get_claim_success(self, repository, sample_claim):
        """Test successful claim retrieval"""
        # Create claim
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:121: AttributeError
_____ TestLanceDBClaimRepository.test_get_claim_not_found _____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EC810>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F781940>

    @pytest.mark.asyncio
    async def test_get_claim_not_found(self, repository):
        """Test retrieving non-existent claim"""
>       retrieved = await repository.get_claim("c9999999")
                          ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'get_claim'

tests\test_lancedb_repositories.py:135: AttributeError
____ TestLanceDBClaimRepository.test_update_claim_success _____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3ED010>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F780140>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_update_claim_success(self, repository, sample_claim):
        """Test successful claim update"""
        # Create claim first
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:142: AttributeError
___ TestLanceDBClaimRepository.test_update_claim_not_found ____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3ED850>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444CF5AE40>

    @pytest.mark.asyncio
    async def test_update_claim_not_found(self, repository):
        """Test updating non-existent claim"""
>       result = await repository.update_claim("c9999999", {"content": "Updated"})
                       ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'update_claim'

tests\test_lancedb_repositories.py:169: AttributeError
_ TestLanceDBClaimRepository.test_update_claim_invalid_fields _

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EE050>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F783040>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_update_claim_invalid_fields(self, repository, sample_claim):
        """Test updating claim with invalid fields"""
        # Create claim first
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:178: AttributeError
____ TestLanceDBClaimRepository.test_delete_claim_success _____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EE850>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F6DF940>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_delete_claim_success(self, repository, sample_claim):
        """Test successful claim deletion"""
        # Create claim first
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:195: AttributeError
________ TestLanceDBClaimRepository.test_search_claims ________

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EF010>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBC840>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_search_claims(self, repository, sample_claims):
        """Test searching claims"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:209: AttributeError
_________ TestLanceDBClaimRepository.test_list_claims _________

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EF810>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444F6DF940>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_list_claims(self, repository, sample_claims):
        """Test listing claims"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:223: AttributeError
__ TestLanceDBClaimRepository.test_list_claims_with_filters ___

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3D4850>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBD240>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_list_claims_with_filters(self, repository, sample_claims):
        """Test listing claims with filters"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:238: AttributeError
________ TestLanceDBClaimRepository.test_vector_search ________

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EEF10>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBDB40>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_vector_search(self, repository, sample_claims):
        """Test vector similarity search"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:266: AttributeError
_____ TestLanceDBClaimRepository.test_get_claims_by_state _____

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EEBD0>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBD640>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_get_claims_by_state(self, repository, sample_claims):
        """Test getting claims by state"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:285: AttributeError
___ TestLanceDBClaimRepository.test_dirty_claims_management ___

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EDF90>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBE440>
sample_claim = Claim(id=c0000001, confidence=0.85, state=Explore)

    @pytest.mark.asyncio
    async def test_dirty_claims_management(self, repository, sample_claim):
        """Test dirty claims management"""
        # Create claim
>       await repository.create_claim(sample_claim)
              ^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:303: AttributeError
__________ TestLanceDBClaimRepository.test_get_stats __________

self = <test_lancedb_repositories.TestLanceDBClaimRepository object at 0x000002444F3EFBD0>
repository = <async_generator object TestLanceDBClaimRepository.repository at 0x000002444FBBED40>
sample_claims = [Claim(id=c00000001, confidence=0.63, state=Explore), Claim(id=c00000002, confidence=0.66, state=Explore), Claim(id=c0...xplore), Claim(id=c00000005, confidence=0.75, state=Explore), Claim(id=c00000006, confidence=0.78, state=Explore), ...]

    @pytest.mark.asyncio
    async def test_get_stats(self, repository, sample_claims):
        """Test getting repository statistics"""
        # Create multiple claims
        for claim in sample_claims:
>           await repository.create_claim(claim)
                  ^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'async_generator' object has no attribute 'create_claim'

tests\test_lancedb_repositories.py:334: AttributeError
_ TestLanceDBRelationshipRepository.test_create_relationship_success _

self = <test_lancedb_repositories.TestLanceDBRelationshipRepository object at 0x000002444F3F4610>
repositories = <async_generator object TestLanceDBRelationshipRepository.repositories at 0x000002444FBBE840>
sample_claims = (Claim(id=c0000001, confidence=0.90, state=Validated), Claim(id=c0000002, confidence=0.95, state=Validated), Claim(id=c0000003, confidence=0.80, state=Explore))

    @pytest.mark.asyncio
    async def test_create_relationship_success(self, repositories, sample_claims):
        """Test successful relationship creation"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:394: TypeError
_ TestLanceDBRelationshipRepository.test_create_relationship_nonexistent_claims _

self = <test_lancedb_repositories.TestLanceDBRelationshipRepository object at 0x000002444F3F4E50>
repositories = <async_generator object TestLanceDBRelationshipRepository.repositories at 0x000002444FBBF640>

    @pytest.mark.asyncio
    async def test_create_relationship_nonexistent_claims(self, repositories):
        """Test creating relationship with non-existent claims"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:411: TypeError
__ TestLanceDBRelationshipRepository.test_get_relationships ___

self = <test_lancedb_repositories.TestLanceDBRelationshipRepository object at 0x000002444F3F5650>
repositories = <async_generator object TestLanceDBRelationshipRepository.repositories at 0x000002444FA78040>
sample_claims = (Claim(id=c0000001, confidence=0.90, state=Validated), Claim(id=c0000002, confidence=0.95, state=Validated), Claim(id=c0000003, confidence=0.80, state=Explore))

    @pytest.mark.asyncio
    async def test_get_relationships(self, repositories, sample_claims):
        """Test getting relationships for a claim"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:422: TypeError
_ TestLanceDBRelationshipRepository.test_get_supporting_claims _

self = <test_lancedb_repositories.TestLanceDBRelationshipRepository object at 0x000002444F3F5E90>
repositories = <async_generator object TestLanceDBRelationshipRepository.repositories at 0x000002444FBBF440>
sample_claims = (Claim(id=c0000001, confidence=0.90, state=Validated), Claim(id=c0000002, confidence=0.95, state=Validated), Claim(id=c0000003, confidence=0.80, state=Explore))

    @pytest.mark.asyncio
    async def test_get_supporting_claims(self, repositories, sample_claims):
        """Test getting supporting claims"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:450: TypeError
_ TestLanceDBRelationshipRepository.test_get_supported_claims _

self = <test_lancedb_repositories.TestLanceDBRelationshipRepository object at 0x000002444F3F6690>
repositories = <async_generator object TestLanceDBRelationshipRepository.repositories at 0x000002444FA78940>
sample_claims = (Claim(id=c0000001, confidence=0.90, state=Validated), Claim(id=c0000002, confidence=0.95, state=Validated), Claim(id=c0000003, confidence=0.80, state=Explore))

    @pytest.mark.asyncio
    async def test_get_supported_claims(self, repositories, sample_claims):
        """Test getting supported claims"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:473: TypeError
____ TestRepositoryIntegration.test_complex_claim_workflow ____

self = <test_lancedb_repositories.TestRepositoryIntegration object at 0x000002444F3F7110>
repositories = <async_generator object TestRepositoryIntegration.repositories at 0x000002444FA79240>

    @pytest.mark.asyncio
    async def test_complex_claim_workflow(self, repositories):
        """Test a complex workflow with claims and relationships"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:507: TypeError
____ TestRepositoryIntegration.test_concurrent_operations _____

self = <test_lancedb_repositories.TestRepositoryIntegration object at 0x000002444F3F7950>
repositories = <async_generator object TestRepositoryIntegration.repositories at 0x000002444FA78D40>

    @pytest.mark.asyncio
    async def test_concurrent_operations(self, repositories):
        """Test concurrent repository operations"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_lancedb_repositories.py:570: TypeError
______ TestLoggingUtilities.test_logger_level_filtering _______

self = <test_monitoring_utilities.TestLoggingUtilities object at 0x000002444F3F4990>

    def test_logger_level_filtering(self):
        """Test logger level filtering"""
        with patch('logging.Logger.debug') as mock_debug, \
             patch('logging.Logger.info') as mock_info:
    
>           logger = get_logger("level_test", level=logging.INFO)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           TypeError: get_logger() got an unexpected keyword argument 'level'

tests\test_monitoring_utilities.py:67: TypeError
__ TestPerformanceMonitor.test_performance_monitor_creation ___

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444DD42050>

    def test_performance_monitor_creation(self):
        """Test performance monitor creation"""
        monitor = PerformanceMonitor()
    
>       assert hasattr(monitor, 'start_timing')
E       AssertionError: assert False
E        +  where False = hasattr(<src.monitoring.performance_monitor.PerformanceMonitor object at 0x000002444FB6C150>, 'start_timing')

tests\test_monitoring_utilities.py:82: AssertionError
__________ TestPerformanceMonitor.test_start_timing ___________

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F4058D0>

    def test_start_timing(self):
        """Test starting timing"""
        monitor = PerformanceMonitor()
    
>       start_time = monitor.start_timing("test_operation")
                     ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:90: AttributeError
___________ TestPerformanceMonitor.test_end_timing ____________

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F405ED0>

    def test_end_timing(self):
        """Test ending timing"""
        monitor = PerformanceMonitor()
    
>       start_time = monitor.start_timing("test_operation")
                     ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:99: AttributeError
________ TestPerformanceMonitor.test_get_metrics_empty ________

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F406510>

    def test_get_metrics_empty(self):
        """Test getting metrics when no operations recorded"""
        monitor = PerformanceMonitor()
    
>       metrics = monitor.get_metrics()
                  ^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'get_metrics'

tests\test_monitoring_utilities.py:110: AttributeError
___ TestPerformanceMonitor.test_get_metrics_with_operations ___

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F406B10>

    def test_get_metrics_with_operations(self):
        """Test getting metrics after operations"""
        monitor = PerformanceMonitor()
    
        # Record some operations
>       monitor.start_timing("op1")
        ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:120: AttributeError
____ TestPerformanceMonitor.test_multiple_same_operations _____

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F407110>

    def test_multiple_same_operations(self):
        """Test multiple executions of same operation"""
        monitor = PerformanceMonitor()
    
        # Run same operation multiple times
        for _ in range(3):
>           monitor.start_timing("repeat_op")
            ^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:143: AttributeError
______ TestPerformanceMonitor.test_timing_without_start _______

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F407D10>

    def test_timing_without_start(self):
        """Test ending timing without starting"""
        monitor = PerformanceMonitor()
    
        # Should handle gracefully
>       duration = monitor.end_timing("nonexistent_operation")
                   ^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'end_timing'

tests\test_monitoring_utilities.py:166: AttributeError
______ TestPerformanceMonitor.test_concurrent_operations ______

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F40C350>

    def test_concurrent_operations(self):
        """Test monitoring concurrent operations"""
        monitor = PerformanceMonitor()
    
        # Start multiple operations
>       start1 = monitor.start_timing("concurrent1")
                 ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:176: AttributeError
__________ TestPerformanceMonitor.test_metrics_reset __________

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F40C910>

    def test_metrics_reset(self):
        """Test resetting metrics"""
        monitor = PerformanceMonitor()
    
        # Add some metrics
>       monitor.start_timing("reset_test")
        ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'start_timing'

tests\test_monitoring_utilities.py:194: AttributeError
______ TestPerformanceMonitor.test_performance_decorator ______

self = <test_monitoring_utilities.TestPerformanceMonitor object at 0x000002444F40CF10>

    def test_performance_decorator(self):
        """Test performance monitoring decorator"""
        monitor = PerformanceMonitor()
    
>       @monitor.monitor_performance
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'PerformanceMonitor' object has no attribute 'monitor_performance'

tests\test_monitoring_utilities.py:208: AttributeError
______ TestProcessLayerModels.test_context_result_model _______

self = <test_process_layer.TestProcessLayerModels object at 0x000002444F463090>

    def test_context_result_model(self):
        """Test ContextResult model creation and validation"""
        claim = Claim(
            id="c0000001",
            content="Test claim",
            confidence=0.8,
            type=[ClaimType.CONCEPT],
            tags=["test"],
            state=ClaimState.EXPLORE,
            scope=ClaimScope.USER_WORKSPACE
        )
    
>       result = ContextResult(
            claim_id="c0000001",
            context_claims=[claim],
            context_size=100,
            traversal_depth=2,
            build_time_ms=50,
            metadata={"test": "value"}
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for ContextResult
E       context_claims.0
E         Input should be a valid dictionary or instance of Claim [type=model_type, input_value=Claim(id=c0000001, confidence=0.80, state=Explore), input_type=Claim]
E           For further information visit https://errors.pydantic.dev/2.5/v/model_type

tests\test_process_layer.py:68: ValidationError
____ TestProcessContextBuilder.test_build_context_success _____

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444DA85F10>
repositories = <async_generator object TestProcessContextBuilder.repositories at 0x000002444FA7A740>
sample_claims = <coroutine object TestProcessContextBuilder.sample_claims at 0x0000024450D219C0>
context_builder = <coroutine object TestProcessContextBuilder.context_builder at 0x000002444FA7B340>

    @pytest.mark.asyncio
    async def test_build_context_success(self, repositories, sample_claims, context_builder):
        """Test successful context building"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:206: TypeError
_ TestProcessContextBuilder.test_build_context_claim_not_found _

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444F4C77D0>
context_builder = <coroutine object TestProcessContextBuilder.context_builder at 0x000002444FA79E40>

    @pytest.mark.asyncio
    async def test_build_context_claim_not_found(self, context_builder):
        """Test context building with non-existent claim"""
        with pytest.raises(ValueError, match="Claim not found"):
>           await context_builder.build_context("c9999999")
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'coroutine' object has no attribute 'build_context'

tests\test_process_layer.py:232: AttributeError
___ TestProcessContextBuilder.test_build_context_with_hints ___

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444F497C50>
repositories = <async_generator object TestProcessContextBuilder.repositories at 0x0000024450D58940>
sample_claims = <coroutine object TestProcessContextBuilder.sample_claims at 0x0000024450D229B0>
context_builder = <coroutine object TestProcessContextBuilder.context_builder at 0x0000024450D58A40>

    @pytest.mark.asyncio
    async def test_build_context_with_hints(self, repositories, sample_claims, context_builder):
        """Test context building with hints"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:237: TypeError
__ TestProcessContextBuilder.test_build_context_depth_limit ___

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444F4D0DD0>
repositories = <async_generator object TestProcessContextBuilder.repositories at 0x0000024450D58740>
sample_claims = <coroutine object TestProcessContextBuilder.sample_claims at 0x0000024450D21CF0>

    @pytest.mark.asyncio
    async def test_build_context_depth_limit(self, repositories, sample_claims):
        """Test context building with depth limit"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:255: TypeError
_______ TestProcessContextBuilder.test_context_caching ________

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444F4D21D0>
repositories = <async_generator object TestProcessContextBuilder.repositories at 0x0000024450D59240>
sample_claims = <coroutine object TestProcessContextBuilder.sample_claims at 0x0000024450D239A0>
context_builder = <coroutine object TestProcessContextBuilder.context_builder at 0x0000024450D59340>

    @pytest.mark.asyncio
    async def test_context_caching(self, repositories, sample_claims, context_builder):
        """Test context result caching"""
>       claim_repo, relationship_repo = repositories
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:277: TypeError
_________ TestProcessContextBuilder.test_clear_cache __________

self = <test_process_layer.TestProcessContextBuilder object at 0x000002444F4E1F90>
context_builder = <coroutine object TestProcessContextBuilder.context_builder at 0x0000024450D59D40>

    @pytest.mark.asyncio
    async def test_clear_cache(self, context_builder):
        """Test clearing context cache"""
        # Add something to cache manually
        claim_id = "c0000001"
>       context_builder._context_cache[claim_id] = ContextResult(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            claim_id=claim_id,
            context_claims=[],
            context_size=0,
            traversal_depth=0
        )
E       AttributeError: 'coroutine' object has no attribute '_context_cache'

tests\test_process_layer.py:297: AttributeError
_____ TestProcessLLMProcessor.test_evaluate_claim_success _____

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4E25D0>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x0000024450E01E10>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_evaluate_claim_success(self, llm_processor, sample_claim):
        """Test successful claim evaluation"""
>       result = await llm_processor.evaluate_claim(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'evaluate_claim'

tests\test_process_layer.py:359: AttributeError
__ TestProcessLLMProcessor.test_evaluate_claim_with_context ___

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4E2DD0>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x0000024450E7C9D0>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_evaluate_claim_with_context(self, llm_processor, sample_claim):
        """Test claim evaluation with context"""
        # Create context with related claims
        context_claims = [
            Claim(
                id="c0000002",
                content="Studies show exercise increases brain-derived neurotrophic factor",
                confidence=0.9,
                type=[ClaimType.REFERENCE],
                tags=["exercise", "bdnf", "neuroscience"],
                state=ClaimState.VALIDATED,
                scope=ClaimScope.USER_WORKSPACE
            )
        ]
    
>       context = ContextResult(
            claim_id=sample_claim.id,
            context_claims=context_claims,
            context_size=200,
            traversal_depth=1
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for ContextResult
E       context_claims.0
E         Input should be a valid dictionary or instance of Claim [type=model_type, input_value=Claim(id=c0000002, confid...e=0.90, state=Validated), input_type=Claim]
E           For further information visit https://errors.pydantic.dev/2.5/v/model_type

tests\test_process_layer.py:384: ValidationError
_ TestProcessLLMProcessor.test_identify_instructions_success __

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4E35D0>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x000002444FC9D710>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_identify_instructions_success(self, llm_processor, sample_claim):
        """Test successful instruction identification"""
>       result = await llm_processor.identify_instructions(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'identify_instructions'

tests\test_process_layer.py:399: AttributeError
_ TestProcessLLMProcessor.test_identify_instructions_with_types _

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4E3E10>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x000002444F6212D0>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_identify_instructions_with_types(self, llm_processor, sample_claim):
        """Test instruction identification with specific types"""
        instruction_types = [InstructionType.VALIDATE_CLAIM, InstructionType.SEARCH_CLAIMS]
    
>       result = await llm_processor.identify_instructions(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            sample_claim,
            instruction_types=instruction_types
        )
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'identify_instructions'

tests\test_process_layer.py:411: AttributeError
_ TestProcessLLMProcessor.test_process_claim_complete_workflow _

self = <src.process.llm_processor.ProcessLLMProcessor object at 0x0000024450E81650>
request = Claim(id=c0000001, confidence=0.75, state=Explore)
context = None

    async def process_claim(
        self,
        request: ProcessingRequest,
        context: Optional[ContextResult] = None
    ) -> ProcessingResult:
        """
        Process a claim with evaluation and instruction identification.
    
        Args:
            request: Processing request containing claim and parameters
            context: Optional pre-built context for the claim
    
        Returns:
            ProcessingResult with evaluation and identified instructions
    
        Raises:
            ValueError: If request is invalid
            RuntimeError: If processing fails
        """
        start_time = datetime.utcnow()
    
        try:
            # Validate request
>           if not request.claim_id:
                   ^^^^^^^^^^^^^^^^

src\process\llm_processor.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Claim(id=c0000001, confidence=0.75, state=Explore)
item = 'claim_id'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra is not None:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Claim' object has no attribute 'claim_id'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:761: AttributeError

During handling of the above exception, another exception occurred:

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4F0650>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x0000024450E81650>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_process_claim_complete_workflow(self, llm_processor, sample_claim):
        """Test complete claim processing workflow"""
>       result = await llm_processor.process_claim(sample_claim)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_process_layer.py:424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.process.llm_processor.ProcessLLMProcessor object at 0x0000024450E81650>
request = Claim(id=c0000001, confidence=0.75, state=Explore)
context = None

    async def process_claim(
        self,
        request: ProcessingRequest,
        context: Optional[ContextResult] = None
    ) -> ProcessingResult:
        """
        Process a claim with evaluation and instruction identification.
    
        Args:
            request: Processing request containing claim and parameters
            context: Optional pre-built context for the claim
    
        Returns:
            ProcessingResult with evaluation and identified instructions
    
        Raises:
            ValueError: If request is invalid
            RuntimeError: If processing fails
        """
        start_time = datetime.utcnow()
    
        try:
            # Validate request
            if not request.claim_id:
                raise ValueError("Claim ID is required in processing request")
    
            # Check cache first
            cache_key = self._generate_cache_key(request)
            if cache_key in self._processing_cache:
                logger.info(f"Returning cached result for claim {request.claim_id}")
                return self._processing_cache[cache_key]
    
            # Update status to in progress
            result = ProcessingResult(
                claim_id=request.claim_id,
                status=ProcessingStatus.IN_PROGRESS,
                created_at=start_time
            )
    
            # Build evaluation prompt
            evaluation_prompt = await self._build_evaluation_prompt(
                request,
                context
            )
    
            # Process with LLM
            llm_response = await self._evaluate_with_llm(evaluation_prompt)
    
            # Parse LLM response
            instructions = await self._parse_instructions(llm_response, request)
            evaluation_score = await self._extract_evaluation_score(llm_response)
            reasoning = await self._extract_reasoning(llm_response)
    
            # Update result with processing outcomes
            processing_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
    
            result.status = ProcessingStatus.COMPLETED
            result.instructions = instructions
            result.evaluation_score = evaluation_score
            result.reasoning = reasoning
            result.processing_time_ms = processing_time_ms
            result.metadata = {
                "cache_key": cache_key,
                "llm_model": getattr(self.llm_bridge, 'model_name', 'unknown'),
                "prompt_tokens": len(evaluation_prompt.split()),
                "instruction_types_found": [inst.instruction_type for inst in instructions]
            }
    
            # Cache successful results
            if result.status == ProcessingStatus.COMPLETED:
                self._processing_cache[cache_key] = result
    
            logger.info(
                f"Processed claim {request.claim_id}: "
                f"score={evaluation_score:.2f}, "
                f"instructions={len(instructions)}, "
                f"time={processing_time_ms}ms"
            )
    
            return result
    
        except Exception as e:
>           logger.error(f"Failed to process claim {request.claim_id}: {str(e)}")
                                                    ^^^^^^^^^^^^^^^^

src\process\llm_processor.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Claim(id=c0000001, confidence=0.75, state=Explore)
item = 'claim_id'

    def __getattr__(self, item: str) -> Any:
        private_attributes = object.__getattribute__(self, '__private_attributes__')
        if item in private_attributes:
            attribute = private_attributes[item]
            if hasattr(attribute, '__get__'):
                return attribute.__get__(self, type(self))  # type: ignore
    
            try:
                # Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
                return self.__pydantic_private__[item]  # type: ignore
            except KeyError as exc:
                raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
        else:
            # `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
            # See `BaseModel.__repr_args__` for more details
            try:
                pydantic_extra = object.__getattribute__(self, '__pydantic_extra__')
            except AttributeError:
                pydantic_extra = None
    
            if pydantic_extra is not None:
                try:
                    return pydantic_extra[item]
                except KeyError as exc:
                    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc
            else:
                if hasattr(self.__class__, item):
                    return super().__getattribute__(item)  # Raises AttributeError if appropriate
                else:
                    # this is the current error
>                   raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E                   AttributeError: 'Claim' object has no attribute 'claim_id'

C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\pydantic\main.py:761: AttributeError
______ TestProcessLLMProcessor.test_process_batch_claims ______

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4F0E90>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x000002444FC9C3D0>

    @pytest.mark.asyncio
    async def test_process_batch_claims(self, llm_processor):
        """Test batch processing of multiple claims"""
        claims = [
            Claim(
                id=f"c000000{i:02d}",
                content=f"Test claim {i} about data science",
                confidence=0.7 + (i * 0.05),
                type=[ClaimType.CONCEPT],
                tags=["test", "data-science"],
                state=ClaimState.EXPLORE,
                scope=ClaimScope.USER_WORKSPACE
            )
            for i in range(1, 6)
        ]
    
        results = await llm_processor.process_batch(claims)
    
        assert len(results) == len(claims)
        for i, result in enumerate(results):
>           assert result.claim_id == claims[i].id
                   ^^^^^^^^^^^^^^^
E           AttributeError: 'AttributeError' object has no attribute 'claim_id'

tests\test_process_layer.py:454: AttributeError
___ TestProcessLLMProcessor.test_llm_bridge_error_handling ____

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4F16D0>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_llm_bridge_error_handling(self, sample_claim):
        """Test handling of LLM bridge errors"""
        # Create mock bridge that raises error
        error_bridge = AsyncMock()
        error_bridge.generate_response.side_effect = Exception("LLM service unavailable")
    
        llm_processor = ProcessLLMProcessor(error_bridge)
    
>       result = await llm_processor.evaluate_claim(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'evaluate_claim'

tests\test_process_layer.py:466: AttributeError
_ TestProcessLLMProcessor.test_instruction_confidence_filtering _

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4F1ED0>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x000002444FC32ED0>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_instruction_confidence_filtering(self, llm_processor, sample_claim):
        """Test filtering of instructions by confidence threshold"""
        # Mock bridge to return low confidence instructions
        llm_processor.llm_bridge.generate_response.return_value = {
            "content": "Analysis complete",
            "instructions": [
                {
                    "type": "validate_claim",
                    "description": "Low priority validation",
                    "confidence": 0.5  # Below threshold
                },
                {
                    "type": "search_claims",
                    "description": "High priority search",
                    "confidence": 0.9  # Above threshold
                }
            ]
        }
    
>       result = await llm_processor.identify_instructions(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'identify_instructions'

tests\test_process_layer.py:491: AttributeError
_______ TestProcessLLMProcessor.test_processing_metrics _______

self = <test_process_layer.TestProcessLLMProcessor object at 0x000002444F4E3D90>
llm_processor = <src.process.llm_processor.ProcessLLMProcessor object at 0x000002444FC6CC50>
sample_claim = Claim(id=c0000001, confidence=0.75, state=Explore)

    @pytest.mark.asyncio
    async def test_processing_metrics(self, llm_processor, sample_claim):
        """Test processing metrics and performance tracking"""
>       result = await llm_processor.evaluate_claim(sample_claim)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'ProcessLLMProcessor' object has no attribute 'evaluate_claim'

tests\test_process_layer.py:500: AttributeError
_ TestProcessLayerIntegration.test_complete_claim_processing_workflow _

self = <test_process_layer.TestProcessLayerIntegration object at 0x000002444F4E3250>
full_system = <async_generator object TestProcessLayerIntegration.full_system at 0x000002444F70E3E0>

    @pytest.mark.asyncio
    async def test_complete_claim_processing_workflow(self, full_system):
        """Test complete workflow from claim creation to processing"""
>       claim_repo, relationship_repo, context_builder, llm_processor = full_system
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:552: TypeError
_ TestProcessLayerIntegration.test_batch_processing_with_relationships _

self = <test_process_layer.TestProcessLayerIntegration object at 0x000002444F4E2510>
full_system = <async_generator object TestProcessLayerIntegration.full_system at 0x000002444F70E510>

    @pytest.mark.asyncio
    async def test_batch_processing_with_relationships(self, full_system):
        """Test batch processing of related claims"""
>       claim_repo, relationship_repo, context_builder, llm_processor = full_system
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:608: TypeError
_ TestProcessLayerIntegration.test_context_building_performance _

self = <test_process_layer.TestProcessLayerIntegration object at 0x000002444F495350>
full_system = <async_generator object TestProcessLayerIntegration.full_system at 0x000002444F70E2B0>

    @pytest.mark.asyncio
    async def test_context_building_performance(self, full_system):
        """Test context building performance with large claim networks"""
>       claim_repo, relationship_repo, context_builder, llm_processor = full_system
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: cannot unpack non-iterable async_generator object

tests\test_process_layer.py:642: TypeError
___ TestRetryUtilities.test_enhanced_retry_config_defaults ____

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4C7BD0>

    def test_enhanced_retry_config_defaults(self):
        """Test EnhancedRetryConfig default values"""
        config = EnhancedRetryConfig()
    
>       assert config.max_attempts == 3
E       AssertionError: assert 5 == 3
E        +  where 5 = EnhancedRetryConfig(max_attempts=5, base_delay=10.0, max_delay=600.0, exponential_base=2.0, jitter=True, jitter_percentage=0.25, rate_limit_multiplier=3.0, network_multiplier=2.0, timeout_multiplier=1.5, connection_multiplier=2.5, http_error_multiplier=1.0, retry_on=[<RetryErrorType.NETWORK_ERROR: 'network_error'>, <RetryErrorType.API_ERROR: 'api_error'>, <RetryErrorType.RATE_LIMIT_ERROR: 'rate_limit_error'>, <RetryErrorType.TIMEOUT_ERROR: 'timeout_error'>, <RetryErrorType.CONNECTION_ERROR: 'connection_error'>, <RetryErrorType.HTTP_ERROR: 'http_error'>, <RetryErrorType.UNKNOWN_ERROR: 'unknown_error'>], non_retryable=[<RetryErrorType.AUTHENTICATION_ERROR: 'authentication_error'>, <RetryErrorType.VALIDATION_ERROR: 'validation_error'>]).max_attempts

tests\test_retry_utilities.py:20: AssertionError
_ TestRetryUtilities.test_with_llm_retry_max_attempts_exceeded _

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F3150>

    def test_with_llm_retry_max_attempts_exceeded(self):
        """Test retry decorator when max attempts exceeded"""
        call_count = 0
    
        @with_llm_retry(max_attempts=3, base_delay=0.01)
        def always_fails():
            nonlocal call_count
            call_count += 1
            raise Exception("Permanent failure")
    
        with pytest.raises(Exception, match="Permanent failure"):
            always_fails()
    
>       assert call_count == 3
E       assert 5 == 3

tests\test_retry_utilities.py:87: AssertionError
-------------------- Captured stderr call ---------------------
WARNING:src.utils.retry_utils:Attempt 1 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 12.05s...
WARNING:src.utils.retry_utils:Attempt 2 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 24.89s...
WARNING:src.utils.retry_utils:Attempt 3 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 41.74s...
WARNING:src.utils.retry_utils:Attempt 4 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 63.96s...
ERROR:src.utils.retry_utils:Non-retryable error (RetryErrorType.UNKNOWN_ERROR): Permanent failure
ERROR:src.utils.retry_utils:All 5 attempts failed. Last error: Permanent failure
---------------------- Captured log call ----------------------
WARNING  src.utils.retry_utils:retry_utils.py:205 Attempt 1 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 12.05s...
WARNING  src.utils.retry_utils:retry_utils.py:205 Attempt 2 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 24.89s...
WARNING  src.utils.retry_utils:retry_utils.py:205 Attempt 3 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 41.74s...
WARNING  src.utils.retry_utils:retry_utils.py:205 Attempt 4 failed (RetryErrorType.UNKNOWN_ERROR): Permanent failure. Retrying in 63.96s...
ERROR    src.utils.retry_utils:retry_utils.py:198 Non-retryable error (RetryErrorType.UNKNOWN_ERROR): Permanent failure
ERROR    src.utils.retry_utils:retry_utils.py:212 All 5 attempts failed. Last error: Permanent failure
_______ TestRetryUtilities.test_retry_delay_calculation _______

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F3750>

    def test_retry_delay_calculation(self):
        """Test retry delay calculation"""
        delays = []
    
>       @with_llm_retry(max_attempts=4, base_delay=0.1, exponential_base=2.0, jitter=False)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: with_llm_retry() got an unexpected keyword argument 'exponential_base'

tests\test_retry_utilities.py:93: TypeError
__________ TestRetryUtilities.test_retry_with_jitter __________

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F3D50>

    def test_retry_with_jitter(self):
        """Test retry with jitter enabled"""
        delays = []
    
>       @with_llm_retry(max_attempts=3, base_delay=0.1, jitter=True)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: with_llm_retry() got an unexpected keyword argument 'jitter'

tests\test_retry_utilities.py:119: TypeError
________ TestRetryUtilities.test_retry_max_delay_limit ________

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F8350>

    def test_retry_max_delay_limit(self):
        """Test retry respects max delay limit"""
        start_time = time.time()
    
>       @with_llm_retry(max_attempts=3, base_delay=10.0, max_delay=0.5, exponential_base=2.0)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: with_llm_retry() got an unexpected keyword argument 'exponential_base'

tests\test_retry_utilities.py:139: TypeError
___ TestRetryUtilities.test_retry_specific_exception_types ____

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F8950>

    def test_retry_specific_exception_types(self):
        """Test retry with specific exception types"""
        call_count = 0
    
>       @with_llm_retry(max_attempts=3, retry_on=(ValueError,))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: with_llm_retry() got an unexpected keyword argument 'retry_on'

tests\test_retry_utilities.py:156: TypeError
_______ TestRetryUtilities.test_retry_config_validation _______

self = <test_retry_utilities.TestRetryUtilities object at 0x000002444F4F8F10>

    def test_retry_config_validation(self):
        """Test retry configuration validation"""
        # Invalid max_attempts
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests\test_retry_utilities.py:177: Failed
===================== Test Suite Summary ======================
[+] Parallel execution: Enabled
[+] Database isolation: Enforced
[+] UTF-8 compliance: Validated
[+] Memory monitoring: Active
[+] Performance timing: Enabled
=================== short test summary info ===================
FAILED tests\test_claim_relationships.py::TestClaimRelationships::test_self_relationship_prevention
FAILED tests\test_e2e_claim_lifecycle_fixed.py::TestClaimLifecycleE2EFixed::test_batch_processing_workflow
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_minimal_configuration_processing
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_performance_configuration_processing
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_local_providers_configuration
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_provider_fallback_and_error_handling
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_driven_batch_sizes
FAILED tests\test_e2e_configuration_driven.py::TestConfigurationDrivenProcessingE2E::test_configuration_memory_and_disk_usage
FAILED tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_claim_network_reasoning
FAILED tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_confidence_propagation
FAILED tests\test_e2e_multiclaim_reasoning.py::TestMultiClaimReasoningE2E::test_batch_reasoning_workflow
FAILED tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestFallbackEvaluation::test_fallback_evaluation_correct
FAILED tests\test_enhanced_glm46_judge.py::TestEnhancedGLM46Judge::TestErrorHandling::test_timeout_handling
FAILED tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_logical_detection
FAILED tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestProblemTypeDetection::test_scientific_detection
FAILED tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestDifficultyEstimation::test_easy_detection
FAILED tests\test_enhanced_prompt_system.py::TestEnhancedPromptSystem::TestResponseParsing::test_mathematical_parsing
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestGSM8KSamples::test_gsm8k_math_problems
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestGSM8KSamples::test_gsm8k_metadata_grade_level
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_structure
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_answer_extraction
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_confidence_calculation
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_correctness_evaluation
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestTaskEvaluation::test_evaluate_task_error_handling
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_run_all_benchmarks_structure
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_run_single_benchmark
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_benchmark_calculation_accuracy
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestBenchmarkSuite::test_benchmark_results_by_type
FAILED tests\test_external_benchmarks.py::TestExternalBenchmarks::TestIntegration::test_prompt_system_integration
FAILED tests\test_id_utilities.py::TestIdGeneration::test_generate_claim_id_format
FAILED tests\test_id_utilities.py::TestIdGeneration::test_validate_claim_id_invalid_format
FAILED tests\test_id_utilities.py::TestIdGeneration::test_validate_claim_id_edge_cases
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_duplicate_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_nonexistent_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_update_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_update_nonexistent_claim
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_create_relationship
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_relationships
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_invalid_claim_validation
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_invalid_confidence
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_large_number_of_claims
FAILED tests\test_lancedb_adapter.py::TestLanceDBAdapterEdgeCases::test_concurrent_operations
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_claim_success
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_claim_validation_error
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_create_duplicate_claim
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claim_success
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claim_not_found
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_success
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_not_found
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_update_claim_invalid_fields
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_delete_claim_success
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_search_claims
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_list_claims
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_list_claims_with_filters
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_vector_search
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_claims_by_state
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_dirty_claims_management
FAILED tests\test_lancedb_repositories.py::TestLanceDBClaimRepository::test_get_stats
FAILED tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_create_relationship_success
FAILED tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_create_relationship_nonexistent_claims
FAILED tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_relationships
FAILED tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_supporting_claims
FAILED tests\test_lancedb_repositories.py::TestLanceDBRelationshipRepository::test_get_supported_claims
FAILED tests\test_lancedb_repositories.py::TestRepositoryIntegration::test_complex_claim_workflow
FAILED tests\test_lancedb_repositories.py::TestRepositoryIntegration::test_concurrent_operations
FAILED tests\test_monitoring_utilities.py::TestLoggingUtilities::test_logger_level_filtering
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_performance_monitor_creation
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_start_timing
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_end_timing
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_get_metrics_empty
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_get_metrics_with_operations
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_multiple_same_operations
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_timing_without_start
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_concurrent_operations
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_metrics_reset
FAILED tests\test_monitoring_utilities.py::TestPerformanceMonitor::test_performance_decorator
FAILED tests\test_process_layer.py::TestProcessLayerModels::test_context_result_model
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_success
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_claim_not_found
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_with_hints
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_build_context_depth_limit
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_context_caching
FAILED tests\test_process_layer.py::TestProcessContextBuilder::test_clear_cache
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_evaluate_claim_success
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_evaluate_claim_with_context
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_identify_instructions_success
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_identify_instructions_with_types
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_process_claim_complete_workflow
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_process_batch_claims
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_llm_bridge_error_handling
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_instruction_confidence_filtering
FAILED tests\test_process_layer.py::TestProcessLLMProcessor::test_processing_metrics
FAILED tests\test_process_layer.py::TestProcessLayerIntegration::test_complete_claim_processing_workflow
FAILED tests\test_process_layer.py::TestProcessLayerIntegration::test_batch_processing_with_relationships
FAILED tests\test_process_layer.py::TestProcessLayerIntegration::test_context_building_performance
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_enhanced_retry_config_defaults
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_with_llm_retry_max_attempts_exceeded
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_retry_delay_calculation
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_retry_with_jitter
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_retry_max_delay_limit
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_retry_specific_exception_types
FAILED tests\test_retry_utilities.py::TestRetryUtilities::test_retry_config_validation
ERROR tests\test_enhanced_glm46_judge.py::TestJudgePerformance::test_evaluation_performance
ERROR tests\test_enhanced_glm46_judge.py::TestJudgePerformance::test_batch_evaluation_performance
ERROR tests\test_enhanced_prompt_system.py::TestPromptSystemPerformance::test_response_time_performance
ERROR tests\test_enhanced_prompt_system.py::TestPromptSystemPerformance::test_memory_efficiency
ERROR tests\test_external_benchmarks.py::TestExternalBenchmarksPerformance::test_suite_execution_performance
ERROR tests\test_external_benchmarks.py::TestExternalBenchmarksPerformance::test_memory_efficiency
ERROR tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_list_claims
ERROR tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_list_claims_with_filters
ERROR tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_search_claims
ERROR tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_vector_search
ERROR tests\test_lancedb_adapter.py::TestLanceDBAdapter::test_get_stats
=== 104 failed, 120 passed, 11 errors in 567.49s (0:09:27) ====
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\_pytest\unraisableexception.py:33: RuntimeWarning: coroutine 'TestProcessContextBuilder.sample_claims' was never awaited
  gc.collect()
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
C:\Users\Aaron.Canary\AppData\Local\Programs\Python\Python311\Lib\site-packages\_pytest\unraisableexception.py:33: RuntimeWarning: coroutine 'TestProcessContextBuilder.context_builder' was never awaited
  gc.collect()
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Pytest finished with code 1
