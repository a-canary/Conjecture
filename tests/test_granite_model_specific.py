"""
Model-Specific Tests for ibm/granite-4-h-tiny
Tests the functionality specifically with the ibm/granite-4-h-tiny model
"""

import unittest
import os
import sys
import json
import re

sys.path.insert(0, "./src")

from conjecture import Conjecture
from core.unified_models import Claim, ClaimType


class TestGraniteModelSpecific(unittest.TestCase):
    """Tests specific to the ibm/granite-4-h-tiny model"""

    @classmethod
    def setUpClass(cls):
        """Set up environment for granite model tests"""
        # Set environment to use LM Studio with granite model
        os.environ["Conjecture_LLM_PROVIDER"] = "lm_studio"
        os.environ["Conjecture_LLM_API_URL"] = "http://127.0.0.1:1234"
        os.environ["Conjecture_LLM_MODEL"] = "ibm/granite-4-h-tiny"

        cls.conjecture = Conjecture()

    def test_model_identification(self):
        """Test that the correct model is being used"""
        self.assertEqual(self.conjecture.config.llm_model, "ibm/granite-4-h-tiny")
        self.assertTrue(self.conjecture.llm_bridge.is_available())

    def test_model_claim_generation_quality(self):
        """Test the quality of claims generated by granite model"""
        # Test various topics to ensure the model responds appropriately
        topics = [
            "machine learning",
            "python programming",
            "data science",
            "artificial intelligence",
        ]

        for topic in topics:
            with self.subTest(topic=topic):
                result = self.conjecture.explore(topic, max_claims=2)

                # Verify that claims are returned
                self.assertGreaterEqual(len(result.claims), 0)

                # Check that claims have reasonable content if any were returned
                for claim in result.claims:
                    self.assertIsInstance(claim, Claim)
                    self.assertGreater(len(claim.content.strip()), 10)
                    self.assertGreaterEqual(claim.confidence, 0.0)
                    self.assertLessEqual(claim.confidence, 1.0)

    def test_model_confidence_reasoning(self):
        """Test that the model provides reasonable confidence scores"""
        # Create a claim with moderate confidence and see if model adjusts appropriately
        content = "Python is a high-level programming language"
        claim = self.conjecture.add_claim(
            content=content,
            confidence=0.75,
            claim_type="concept",
            validate_with_llm=True,
        )

        # The model should return a confidence score that makes sense
        self.assertGreaterEqual(claim.confidence, 0.0)
        self.assertLessEqual(claim.confidence, 1.0)

        # Even if validation changes the confidence, it should be reasonable
        print(
            f"Original confidence: 0.75, Model-validated confidence: {claim.confidence}"
        )

    def test_model_response_format(self):
        """Test that the model returns properly formatted responses"""
        result = self.conjecture.explore("computer science fundamentals", max_claims=1)

        for claim in result.claims:
            # Check that content is meaningful and well-formed
            self.assertGreater(len(claim.content.strip()), 10)

            # Check that the claim has proper type
            self.assertIsNotNone(claim.type)
            self.assertGreaterEqual(len(claim.type), 1)
            self.assertIn(claim.type[0], list(ClaimType))

    def test_model_various_claim_types(self):
        """Test that the model handles different claim types appropriately"""
        test_cases = [
            ("concept", "A variable is a storage location in programming"),
            ("example", "An example of a Python list is [1, 2, 3]"),
            (
                "thesis",
                "Programming efficiency can be improved through proper algorithm selection",
            ),
            ("goal", "Learn Python programming fundamentals"),
        ]

        for claim_type, content in test_cases:
            with self.subTest(claim_type=claim_type):
                claim = self.conjecture.add_claim(
                    content=content,
                    confidence=0.80,
                    claim_type=claim_type,
                    validate_with_llm=True,
                )

                # Verify the claim type is preserved
                self.assertEqual(claim.type[0].value, claim_type)

                # Verify content is meaningful
                self.assertGreater(len(claim.content.strip()), 10)

    def test_model_context_understanding(self):
        """Test that the model understands context in exploration"""
        # Test with a more specific query to see if model provides relevant results
        result = self.conjecture.explore("Python list operations", max_claims=3)

        relevant_keywords = [
            "list",
            "python",
            "[]",
            "elements",
            "index",
            "append",
            "iteration",
        ]

        relevant_claims = 0
        for claim in result.claims:
            claim_text = claim.content.lower()
            if any(keyword in claim_text for keyword in relevant_keywords):
                relevant_claims += 1

        # At least half of the claims should be related to the topic
        self.assertGreaterEqual(
            relevant_claims,
            len(result.claims) // 2,
            "Most claims should be relevant to the query",
        )


def run_granite_model_tests():
    """Run the granite model-specific tests"""
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestGraniteModelSpecific)

    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    return result


if __name__ == "__main__":
    print("Running ibm/granite-4-h-tiny model-specific tests...")
    print(
        "Ensure LM Studio is running at http://127.0.0.1:1234 with ibm/granite-4-h-tiny model"
    )
    run_granite_model_tests()
