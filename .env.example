# ===========================================
# CONJECTURE UNIFIED CONFIGURATION
# ===========================================
# Copy this file to .env and configure your provider
# Choose ONE provider section below and uncomment it

# ===========================================
# LOCAL PROVIDERS (Recommended for Privacy)
# ===========================================

# Ollama - Local LLM Server
# Install: https://ollama.ai/ | Start: ollama serve | Pull: ollama pull llama2
#PROVIDER_API_URL=http://localhost:11434
#PROVIDER_API_KEY=
#PROVIDER_MODEL=llama2

# LM Studio - Local GUI LLM Server
# Install: https://lmstudio.ai/ | Start: Launch LM Studio app
#PROVIDER_API_URL=http://localhost:1234
#PROVIDER_API_KEY=
#PROVIDER_MODEL=ibm/granite-4-h-tiny

# ===========================================
# CLOUD PROVIDERS (Fast Setup)
# ===========================================

# Chutes.ai - Fast & Cost Effective
# Get key: https://chutes.ai/
PROVIDER_API_URL=https://llm.chutes.ai/v1
PROVIDER_API_KEY=cpk_your-api-key-here
PROVIDER_MODEL=zai-org/GLM-4.6-FP8

# OpenRouter - 100+ Models
# Get key: https://openrouter.ai/keys
#PROVIDER_API_URL=https://openrouter.ai/api/v1
#PROVIDER_API_KEY=sk-or-your-api-key-here
#PROVIDER_MODEL=openai/gpt-3.5-turbo

# OpenAI - Most Popular
# Get key: https://platform.openai.com/api-keys
#PROVIDER_API_URL=https://api.openai.com/v1
#PROVIDER_API_KEY=sk-your-api-key-here
#PROVIDER_MODEL=gpt-3.5-turbo

# ===========================================
# APPLICATION SETTINGS
# ===========================================

# Database
DB_PATH=data/conjecture.db

# Performance
CONFIDENCE_THRESHOLD=0.95
MAX_CONTEXT_SIZE=10
BATCH_SIZE=10

# Development
DEBUG=false

# ===========================================
# QUICK START
# ===========================================
# 1. Uncomment ONE provider above
# 2. Replace YOUR_API_KEY with actual key
# 3. Save as .env
# 4. Run: python demo/simple_conjecture_cli.py validate